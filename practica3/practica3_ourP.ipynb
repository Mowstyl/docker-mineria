{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 3 - Multiclasificadores y selección de variables\n",
    "\n",
    "## Minería de Datos 2017/2018\n",
    "\n",
    "* [**Hernán Indíbil de La Cruz Calvo**](https://github.com/Mowstyl)\n",
    "* [**Alejandro Martín Simón Sánchez**](https://github.com/elssbbboy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice\n",
    "1) [Introducción](#1)\n",
    "\n",
    "2) [Ensembles](#2)\n",
    "   \n",
    "- 2.1 [Implementación básica de un ensemble de manera manual](#21)\n",
    "   \n",
    "- 2.2 [Bagging](#22)\n",
    "\n",
    "- 2.3 [Random Forest](#23)\n",
    "\n",
    "- 2.4 [Boosting](#24)\n",
    "\n",
    "- 2.5 [Gradient Boosting Trees](#25)\n",
    "\n",
    "- 2.6 [Comparación](#26)\n",
    "\n",
    "3) [Filtrado](#3)\n",
    "\n",
    "4) [Wrapper](#4)\n",
    "\n",
    "5) [Algoritmo de selección de parámetros voraz](#5)\n",
    "\n",
    "\n",
    "## 1. Introducción <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta práctica se divide en dos partes:\n",
    "\n",
    "**Multiclasificadores:** estudiaremos la API de scikit para los modelos de tipo ensemble y veremos como entrenar, seleccionar y utilizar estos modelos.\n",
    "\n",
    "**Selección de variables:** veremos los distintos algoritmos de selección de variables disponibles en scikit y como aplicarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always load all scipy stack packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, integrate\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code configures matplotlib for proper rendering\n",
    "%matplotlib inline\n",
    "mpl.rcParams[\"figure.figsize\"] = \"8, 4\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=6470\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'target', 'target_names']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_diabetes\n",
    "dss = {}\n",
    "dss['wisconsin'] = load_breast_cancer()\n",
    "dss['pima'] = load_diabetes()\n",
    "dir(dss['wisconsin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, en dss['wisconsin'] tenemos los siguientes atributos:\n",
    "\n",
    "* DESCR: descripción del dataset, con numero de casos, codificación de los valores perdidos, atributos, información de los mismos además de estadísticos y información adicional sobre la obtención de los datos y su origen.\n",
    "* data: array de numpy con los valores de cada atributo para cada caso. En prácticas anteriores sería similar al train_atts o test_atts (aún no hemos hecho holdout), cambiando que no se utiliza pandas y no tenemos los nombres de las variables predictoras.\n",
    "* feature_names: nombres de las variables categóricas, usado como columna en pandas anteriormente. De esta forma, dfs['wisconsin'].data[n][m] indica un valor de la variable dfs['wisconsin'].feature_names[m].\n",
    "* target: array de numpy con los valores de la clase para cada instancia. No se trabaja con strings, sino que ha sido binarizada para trabajar con 0 y 1 (más eficiente que trabajar con strings).\n",
    "* target_names: nombres de los distintos valores que toma la variable categórica original. De esta forma si tenemos un 0 en target sabemos que tenemos que se clasifica como target_names[0], en este caso 'malignant'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'feature_names', 'target']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dss['pima'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, en dss['pima'] tenemos algunas diferencias con el dataset wisconsin:\n",
    "\n",
    "* En target ya no tenemos 0 y 1, ya que la variable clase no es categórica, sino una medida cuantitativa de la progresión de la enfermedad un año después de la baseline.\n",
    "* El atributo target_names no tiene sentido ya que no existe ninguna correspondencia como en wisconsin. La variable clase no es categórica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dssh = {}\n",
    "\n",
    "for ds in dss:\n",
    "    strat = None\n",
    "    if ('target_names' in dir(dss[ds])): # Solo tiene sentido estratificar si la clase es categórica\n",
    "        strat = dss[ds].target\n",
    "    \n",
    "    dssh[ds] = {'train': {}, 'test': {}}\n",
    "    dssh[ds]['train']['atts'], dssh[ds]['test']['atts'], dssh[ds]['train']['label'], dssh[ds]['test']['label'] = train_test_split(\n",
    "        dss[ds].data,\n",
    "        dss[ds].target,\n",
    "        random_state = seed,\n",
    "        stratify = strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función lleva a cabo un muestreo de variables predictoras. Los parámetros que se le pueden pasar son:\n",
    "* atts: array con los casos, preferiblemente de numpy. train_atts.\n",
    "* label: array con la variable clase asociada a cada caso. train_label.\n",
    "* feature_names: nombres de las variables predictoras. Opcional.\n",
    "* n_samples: número de muestras a generar. Opcional.\n",
    "* best_features: parámetro que indica si se tomarán las mejores variables para cada muestra. Si hay reemplazo no tiene sentido, ya que las n muestras que genere serán idénticas. Opcional.\n",
    "* replace: parámetro que indica si el muestreo se realiza con reemplazo. Opcional.\n",
    "* random_state: parámetro que indica la semilla para generar números aleatorios, solo utilizada con best_features=False. Opcional.\n",
    "* k: número de variables en cada muestra en caso de haber reemplazo y que best_features=False. Opcional.\n",
    "\n",
    "Devuelve una tripla con:\n",
    "* Lista de muestras(solo los casos/atts).\n",
    "* Lista de muestras(solo la clase/label).\n",
    "* Lista de listas con nombres de variables predictoras usadas en cada muestra. Si no se especificó feature_names devuelve None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Ensembles <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1  Implementación básica de un ensemble de manera manual <a class=\"anchor\" id=\"21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def sampleFeatures(atts, label, feature_names=None, n_samples=4, best_features=True, replace=False,\n",
    "                   random_state=None, k=10):\n",
    "    if (random_state is not None and not best_features):\n",
    "        random.seed(random_state)\n",
    "    kaux = int(atts.shape[1]/n_samples)\n",
    "    ks = [kaux for i in range(0, n_samples)]\n",
    "    if (kaux * n_samples < atts.shape[1]):\n",
    "        ks[-1] += 1\n",
    "    \n",
    "    auxatts = atts[:,:]\n",
    "    fnames = None\n",
    "    if (feature_names is not None):\n",
    "        auxfname = feature_names[:]\n",
    "        fnames = []\n",
    "    \n",
    "    sample = []\n",
    "    labels = []\n",
    "    \n",
    "    for s in ks:\n",
    "        if (best_features):\n",
    "            sel = SelectKBest(mutual_info_classif, k=s)\n",
    "            sel.fit(auxatts, label)\n",
    "            mask = sel.get_support()\n",
    "        else:\n",
    "            cols = s\n",
    "            if (replace):\n",
    "                cols = k\n",
    "            samp = random.sample(range(0, auxatts.shape[1]), cols)\n",
    "            mask = [i in samp for i in range(0, auxatts.shape[1])]\n",
    "        sample.append(auxatts[:,mask])\n",
    "        labels.append(label)\n",
    "        if (feature_names is not None):\n",
    "            fnames.append(auxfname[mask])\n",
    "            if(not replace):\n",
    "                auxfname = auxfname[np.invert(mask)]\n",
    "        if(not replace):\n",
    "            auxatts = auxatts[:,np.invert(mask)]\n",
    "    return np.array(sample), np.array(labels), np.array(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función lleva a cabo el muestreo, sobre variables o sobre casos, con o sin reemplazo. Los parámetros que se le pueden pasar son:\n",
    "* atts: array con los casos, preferiblemente de numpy. train_atts.\n",
    "* label: array con la variable clase asociada a cada caso. train_label.\n",
    "* feature_names: nombres de las variables predictoras. Opcional.\n",
    "* n_samples: número de muestras a generar. Opcional.\n",
    "* sample_features: indica si el muestreo se realiza sobre los casos (False) o sobre las variables predictoras (True)\n",
    "* best_features: parámetro que indica si se tomarán las mejores variables para cada muestra. Si hay reemplazo no tiene sentido, ya que las n muestras que genere serán idénticas. Solo se toma si sample_features=True. Opcional.\n",
    "* replace: parámetro que indica si el muestreo se realiza con reemplazo (sobre las variables predictoras o sobre los casos según el valor de sample_features). Opcional.\n",
    "* random_state: parámetro que indica la semilla para generar números aleatorios, solo utilizada con best_features=False. Opcional.\n",
    "* k: número de variables en cada muestra en caso de haber reemplazo, sample_features=True y best_features=False. Opcional.\n",
    "\n",
    "Devuelve una tripla con:\n",
    "* Lista de muestras(solo los casos/atts).\n",
    "* Lista de muestras(solo la clase/label).\n",
    "* Lista de listas con nombres de variables predictoras usadas en cada muestra. Si no se especificó feature_names devuelve None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def ourSample(atts, label, feature_names=None, random_state=None, n_samples=2,\n",
    "              replace=True, sample_features=False, best_features=True, k=10):\n",
    "    if (sample_features):\n",
    "        return sampleFeatures(atts, label, feature_names=feature_names, random_state=random_state,\n",
    "                              best_features=best_features, n_samples=n_samples, replace=replace, k=k)\n",
    "    elif(replace):\n",
    "        sample = []\n",
    "        labels = []\n",
    "        fnames = None\n",
    "        if (feature_names is not None):\n",
    "            fnames = []\n",
    "        for i in range(0, n_samples):\n",
    "            X, y = resample(atts, label, random_state=random_state, n_samples=n_samples, replace=True)\n",
    "            sample.append(X)\n",
    "            labels.append(y)\n",
    "            if (feature_names is not None):\n",
    "                fnames.append(feature_names)\n",
    "        return np.array(sample), np.array(labels), np.array(fnames)\n",
    "    else:\n",
    "        if (random_state is not None):\n",
    "            random.seed(random_state)\n",
    "        ncases = int(atts.shape[0]/n_samples)\n",
    "        ncss = [ncases for i in range(0, n_samples)]\n",
    "        if (ncases * n_samples < atts.shape[0]):\n",
    "            ncss[-1] += 1\n",
    "        auxatts = atts[:,:]\n",
    "        auxlabels = label[:]\n",
    "        sample = []\n",
    "        labels = []\n",
    "        fnames = None\n",
    "        if (feature_names is not None):\n",
    "            fnames = []\n",
    "        for nc in ncss:\n",
    "            samp = random.sample(range(0, auxatts.shape[0]), nc)\n",
    "            mask = [i in samp for i in range(0, auxatts.shape[0])]\n",
    "            sample.append(auxatts[mask])\n",
    "            labels.append(auxlabels[mask])\n",
    "            if (feature_names is not None):\n",
    "                fnames.append(feature_names)\n",
    "            auxatts = auxatts[np.invert(mask)]\n",
    "            auxlabels = auxlabels[np.invert(mask)]\n",
    "        return np.array(sample), np.array(labels), np.array(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de uso:\n",
    "Vamos a generar una lista de 10 muestras en las que muestreemos las variables predictoras. Cada muestra tendrá 10 variables predictoras escogidas aleatoriamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, l, n = ourSample(dssh['wisconsin']['train']['atts'],\n",
    "                    dssh['wisconsin']['train']['label'],\n",
    "                    feature_names=dss['wisconsin'].feature_names, n_samples=10,\n",
    "                    replace=True, best_features=False, sample_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, en a[0] tenemos nuestra primera muestra, donde a[0][0] sería el primer caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.74100000e+01,   2.99500000e-02,   1.20100000e-02,\n",
       "         6.48100000e-02,   8.61400000e-03,   2.71000000e-02,\n",
       "         3.45100000e-03,   2.33100000e+01,   7.42200000e+01,\n",
       "         7.98700000e-02])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en l[0][0] se tendrá el valor de la clase para el primer caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En n[0] se tendrá el nombre de las variables predictoras usadas en la primera muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean perimeter', 'mean concavity', 'mean concave points',\n",
       "       'mean fractal dimension', 'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst texture', 'worst perimeter',\n",
       "       'worst concavity'],\n",
       "      dtype='<U23')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez listo el método para realizar el muestreo, podemos proceder a diseñar el ensemble.\n",
    "Tomará como parámetros los necesarios para ajustar el algoritmo del muestreo además de el algoritmo que se usará para el ensemble. Dicho algoritmo vendrá dado con una clase cuyas instancias tengan implementados los métodos set_params, fit y predict. Además, se podrá dar otro parámetro al ensemble llamado params que se podrá usar para indicar los parámetros que se pasan a todas las instancias del algoritmo al crear el ensemble.\n",
    "Cabe destacar que en lugar de n_samples tendremos n_models, que indicará tanto el número de muestras como el número de estimadores generados.\n",
    "El ensemble implementará a su vez los métodos fit y predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "class EnsembleHome:\n",
    "    def __init__(self, est, random_state=None, n_models=10, replace=True,\n",
    "                 sample_features=False, best_features=True, k=10, params=None):\n",
    "        self.base = est\n",
    "        self.rstate = random_state\n",
    "        self.nmodels = n_models\n",
    "        self.replace = replace\n",
    "        self.sfeatures = sample_features\n",
    "        self.bfeatures = best_features\n",
    "        self.k = k\n",
    "        self.params = params\n",
    "        \n",
    "        self.ests = []\n",
    "        for i in range(0, n_models):\n",
    "            self.ests.append(est())\n",
    "            if (params is not None):\n",
    "                self.ests[i].set_params(**params)\n",
    "        \n",
    "    def fit(self, atts, label, feature_names=None):\n",
    "        self.fnames = feature_names\n",
    "        satts, slabels, sfname = ourSample(atts, label, feature_names=feature_names,\n",
    "                                           n_samples=self.nmodels, replace=self.replace, random_state=self.rstate,\n",
    "                                           best_features=self.bfeatures, sample_features=self.sfeatures)\n",
    "        self.satts = satts\n",
    "        self.slabels = slabels\n",
    "        self.sfname = sfname\n",
    "        for i in range(0, self.nmodels):\n",
    "            self.ests[i].fit(self.satts[i], self.slabels[i])\n",
    "    \n",
    "    def predict(self, atts):\n",
    "        predictions = []\n",
    "        for i in range(len(self.ests)):\n",
    "            mask = [self.fnames[j] in self.sfname[i] for j in range(len(self.fnames))]\n",
    "            paux = self.ests[i].predict(atts[:,mask])\n",
    "            predictions.append(paux)\n",
    "        return stats.mode(predictions)[0][0]\n",
    "        #return np.stats.mode(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Muestreo con reemplazo y sin muestreo de atributos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "ensemble = EnsembleHome(tree.DecisionTreeClassifier, random_state=seed, n_models=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'], feature_names=dss['wisconsin'].feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {}\n",
    "p['Con reemplazo, sin muestreo de atributos'] = ensemble.predict(dssh['wisconsin']['test']['atts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A continuación se realiza el muestreo sin reemplazo y sin muestreo de atributos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleHome(tree.DecisionTreeClassifier, replace = False, random_state=seed, n_models=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'], feature_names=dss['wisconsin'].feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Sin reemplazo, sin muestreo de atributos'] = ensemble.predict(dssh['wisconsin']['test']['atts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lo siguiente es realizar el muestreo sin reemplazo y con muestreo de atributos.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleHome(tree.DecisionTreeClassifier, replace = False, sample_features=True, best_features=False, random_state=seed, n_models=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'], feature_names=dss['wisconsin'].feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Sin reemplazo, con muestreo de atributos'] = ensemble.predict(dssh['wisconsin']['test']['atts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Muestro con reemplazo y con muestreo de atributos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleHome(tree.DecisionTreeClassifier, replace = True, sample_features=True, best_features=False, random_state=seed, n_models=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'], feature_names=dss['wisconsin'].feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Con reemplazo, con muestreo de atributos'] = ensemble.predict(dssh['wisconsin']['test']['atts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conjuntos con los mejores atributos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleHome(tree.DecisionTreeClassifier, replace = False, sample_features=True, best_features=True, random_state=seed, n_models=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'], feature_names=dss['wisconsin'].feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Conjuntos con los mejores atributos'] = ensemble.predict(dssh['wisconsin']['test']['atts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# Metodo que pasado test_label y un diccionario con predicciones indizadas\n",
    "#   por el algoritmo devuelve una tabla con las metricas utilizadas\n",
    "\n",
    "def metricTable(test_label, predictions, pos_label):\n",
    "    algL = []\n",
    "    accuracyL = []\n",
    "    recallL = []\n",
    "    precisionL = []\n",
    "    f1scoreL = []\n",
    "    aucL = []\n",
    "\n",
    "    for alg, prediction in predictions.items():\n",
    "        algL.append(alg)\n",
    "        accuracyL.append(metrics.accuracy_score(test_label, prediction))\n",
    "        recallL.append(metrics.recall_score(test_label, prediction, pos_label=pos_label))\n",
    "        precisionL.append(metrics.precision_score(test_label, prediction, pos_label=pos_label))\n",
    "        f1scoreL.append(metrics.f1_score(test_label, prediction, pos_label=pos_label))\n",
    "        aucL.append(metrics.roc_auc_score(y_true=pd.get_dummies(test_label), y_score=pd.get_dummies(prediction)))\n",
    "\n",
    "    table = [('Algorithm', algL),\n",
    "             ('Accuracy', accuracyL),\n",
    "             ('Recall', recallL),\n",
    "             ('Precision', precisionL),\n",
    "             ('F1 Score', f1scoreL),\n",
    "             ('ROC AUC', aucL)\n",
    "             ]\n",
    "\n",
    "    return pd.DataFrame.from_items(table)\n",
    "\n",
    "def cMatrix(matrix, pos_label, neg_label):\n",
    "    rowL = [pos_label, neg_label]\n",
    "    tColL = [matrix[1,1], matrix[0,1]]\n",
    "    fColL = [matrix[1,0], matrix[0,0]]\n",
    "    table = [('Actual \\ Pred', rowL),\n",
    "             (pos_label, tColL),\n",
    "             (neg_label, fColL)]\n",
    "    return pd.DataFrame.from_items(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Con reemplazo, sin muestreo de atributos</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.854369</td>\n",
       "      <td>0.881761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sin reemplazo, sin muestreo de atributos</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.902306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sin reemplazo, con muestreo de atributos</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.922851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Con reemplazo, con muestreo de atributos</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.912621</td>\n",
       "      <td>0.926730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conjuntos con los mejores atributos</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.891195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Algorithm  Accuracy    Recall  Precision  \\\n",
       "0  Con reemplazo, sin muestreo de atributos  0.895105  0.830189   0.880000   \n",
       "1  Sin reemplazo, sin muestreo de atributos  0.916084  0.849057   0.918367   \n",
       "2  Sin reemplazo, con muestreo de atributos  0.937063  0.867925   0.958333   \n",
       "3  Con reemplazo, con muestreo de atributos  0.937063  0.886792   0.940000   \n",
       "4       Conjuntos con los mejores atributos  0.902098  0.849057   0.882353   \n",
       "\n",
       "   F1 Score   ROC AUC  \n",
       "0  0.854369  0.881761  \n",
       "1  0.882353  0.902306  \n",
       "2  0.910891  0.922851  \n",
       "3  0.912621  0.926730  \n",
       "4  0.865385  0.891195  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricTable(dssh['wisconsin']['test']['label'], p, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bagging  <a class=\"anchor\" id=\"22\"></a>\n",
    "\n",
    "La estrategia básica tras el algoritmo de **bagging** es la agregación de distintos clasificadores que han sido aprendidos a partir de muestras obtenidas tras un proceso de bootstraping (muestreo con remplazo). Aquí estudiaremos cómo aprender un modelo de este tipo utilizando scikit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sklearn.ensemble.BaggingClassifier(base_estimator=None, (El clasificador de base que usará el ensemble) \n",
    "                                   n_estimators=10, (Numero de modelos que aprenderemos) \n",
    "                                   max_samples=1.0, (proporción de la muestra a utilizar)\n",
    "                                   max_features=1.0, (proporcion de características a  utilizar)\n",
    "                                   bootstrap=True, (si se utilizará muestreo por remplazo o no)\n",
    "                                   n_jobs=1, (para utilizar paralelismo, debe soportarlo nuestro entorno)\n",
    "                                   random_state=None (la semilla)\n",
    "                                  )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagg = BaggingClassifier(tree.DecisionTreeClassifier(), \n",
    "                          n_estimators = 30, \n",
    "                          random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "scores_bagg = cross_val_score(bagg, dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'], cv=3, scoring=\"accuracy\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_bagg.mean(), scores_bagg.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Random Forest <a class=\"anchor\" id=\"23\"></a>\n",
    "Una alternativa muy popular al bagging es el clasificador **random forest**. En este caso también se utiliza muestreo con remplazo, pero adicionalmente integra otras técnicas de aleatorización en el aprendizaje de los árboles de decisión para ampliar la generalización del ensemble. Concretamente utiliza una muestra aleatoria de los atributos a la hora de seleccionar cada punto óptimo de corte.\n",
    "\n",
    "Un clasificador random forest siempre utiliza árboles de decisión como submodelos y por ello no requiere un clasificador base para su definición. Por esa misma razón los hyperparámetros de este clasificador incluyen tanto los correspondientes al ensemble como al árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sklearn.ensemble.RandomForestClassifier(n_estimators=10, (numero de modelos que aprenderemos)\n",
    "                                        criterion='gini', (parametro del arbol de decision)\n",
    "                                        max_depth=None, (parametro del arbol de decision)\n",
    "                                        min_samples_split=2, (parametro del arbol de decision)\n",
    "                                        min_samples_leaf=1, (parametro del arbol de decision)\n",
    "                                        min_impurity_split=1e-07, (parametro del arbol de decision)\n",
    "                                        max_features='auto', (número de atributos que se usan en cada caso:\n",
    "                                                              puede ser un entero para número exacto, un float\n",
    "                                                              para proporcion o 'auto', 'sqrt', 'log2' o 'None')\n",
    "                                        bootstrap=True, (si se utilizará muestreo por remplazo)\n",
    "                                        n_jobs=1, (para utilizar paralelismo, debe soportarlo nuestro entorno)\n",
    "                                        random_state=None, (semilla)\n",
    "                                        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=30, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "scores_rf = cross_val_score(rf, dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'], cv=3, scoring=\"accuracy\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_rf.mean(), scores_rf.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Boosting <a class=\"anchor\" id=\"24\"></a>\n",
    "La estrategia de boosting es completamente diferente a la anterior, ya que en lugar de reducir la varianza al aprender múltiples clasificadores independientes, realizaremos un proceso iterativo en el que intensificaremos el aprendizaje sobre aquellas instancias más complicadas de aprender para nuestro modelo. \n",
    "El algoritmo más conocido se llama `AdaBoost` y está disponible en scikit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "sklearn.ensemble.AdaBoostClassifier(base_estimator=None, (El clasificador de base que usará el ensemble) \n",
    "                                    n_estimators=50, (numero de modelos que aprenderemos)\n",
    "                                    learning_rate=1.0, (la contribución del modelo anterior al siguiente)\n",
    "                                    random_state=None (semilla)\n",
    "                                    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "boost = AdaBoostClassifier(base_estimator=tree.DecisionTreeClassifier(), n_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "scores_boost = cross_val_score(boost, dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'], cv=3, scoring=\"accuracy\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_boost.mean(), scores_boost.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Gradient Boosting Trees <a class=\"anchor\" id=\"25\"></a>\n",
    "Es una generalización del algoritmo de boosting que optimiza funciones de coste en el dominio concreto de los árboles de decisión. Actualmente es una de las técnicas más exitosas para resolver problemas de clasificación complejos *off of the shelf*.\n",
    "\n",
    "```\n",
    "sklearn.ensemble.GradientBoostingClassifier(n_estimators=100, (numero de modelos que aprenderemos)\n",
    "                                            learning_rate=0.1, (la contribución del modelo anterior)\n",
    "                                            subsample=1.0, (aprender mediante submuestreo)\n",
    "                                            max_features=None, (submuestrear atributos como en RF)\n",
    "                                            min_samples_split=2, (parametro del arbol de decision)\n",
    "                                            min_samples_leaf=1, (parametro del arbol de decision)\n",
    "                                            max_depth=3, (parametro del arbol de decision)\n",
    "                                            min_impurity_split=1e-07, (parametro del arbol de decision)\n",
    "                                            random_state=None, (semilla)\n",
    "                                            )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gboost = GradientBoostingClassifier(n_estimators=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "scores_gboost = cross_val_score(gboost, dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'], cv=5, scoring=\"accuracy\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_gboost.mean(), scores_gboost.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.6 Comparación <a class=\"anchor\" id=\"26\"></a>\n",
    "Cualquier técnica de ensemble debería ser capaz de mejorar el rendimiento del clasificador base, aunque el rendimiento entre ellas puede depender enormemente del **problema** y de los **parámetros** con los que se hayan configurado las técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def printTable(list):\n",
    "    table = \"\"\"<table>%s</table>\"\"\"\n",
    "    row = \"\"\"<tr>%s</tr>\"\"\"\n",
    "    cell = \"\"\"<td>%s</td>\"\"\"\n",
    "    report =  table % ''.join([row % (cell % x[0] + cell % x[1]) for x in results])\n",
    "    display(HTML(report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Bagging</td><td>0.957746478873</td></tr><tr><td>Random Forest</td><td>0.948356807512</td></tr><tr><td>Boosting</td><td>0.915492957746</td></tr><tr><td>Gradient Boosting</td><td>0.95072307993</td></tr><tr><td>Casero - Con reemplazo, sin muestreo de atributos</td><td>0.895104895105</td></tr><tr><td>Casero - Sin reemplazo, sin muestreo de atributos</td><td>0.916083916084</td></tr><tr><td>Casero - Sin reemplazo, con muestreo de atributos</td><td>0.937062937063</td></tr><tr><td>Casero - Con reemplazo, con muestreo de atributos</td><td>0.937062937063</td></tr><tr><td>Casero - Conjuntos con los mejores atributos</td><td>0.902097902098</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = [(\"Bagging\", scores_bagg.mean()), \n",
    "           (\"Random Forest\", scores_rf.mean()),\n",
    "           (\"Boosting\", scores_boost.mean()),\n",
    "           (\"Gradient Boosting\", scores_gboost.mean())]\n",
    "results += [(\"Casero - \" + alg, metrics.accuracy_score(dssh['wisconsin']['test']['label'], p[alg])) for alg in p]\n",
    "printTable(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategias de reducción de varianza vs estrategias de reducción de sesgo (variance and bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque la estrategia común detrás de un ensemble sea la de generar un consenso entre varios modelos (mediante agregación o voto por ejemplo), la forma en la que se genera el conjunto de modelos está dirigida por motivaciones muy diferentes para algoritmos como bagging o boosting. Cuando utilizamos una u otra técnica es fundamental conocer el fundamento interno de la técnica de ensembling para seleccionar y aprender correctamente los modelos del ensemble.\n",
    "\n",
    "En el caso de **bagging** o **random forest** queremos utilizar una función de aprendizaje (C4.5, CART...) y obtener modelos \"diversos\" a partir de los mismos datos para reducir el error obtenido mediante **varianza**. Formalmente hablamos de clasificadores que individualmente tengan poder de generalización pero que al mismo tiempo estén lo menos correlados entre ellos. Es por ello que se utilizan diversas técnicas de submuestreo y aleatorización de los modelos. \n",
    "\n",
    "En el caso de los algoritmos de **boosting** la estrategia es opuesta, dado un problema complejo queremos reducir el sesgo en nuestros modelos. Por ello en lugar de aleatorización guíamos la función de aprendizaje para que incida en aquellos ejemplos que sean más dificiles de generalizar. La técnica general es utilizar sobremuestreo o pesos para que sobreajustar el clasificador en aquellos casos conflictivos, de manera más formal o sofisticada se puede establecer un problema de optimización de funciones de coste.\n",
    "\n",
    "___\n",
    "Es por ello que para el **bagging** es interesante utilizar 'strong learners' clasificadores con un gran poder predictivo en sus parámetros que den lugar a **modelos con mucha varianza, ya que la varianza se reduce.**\n",
    "\n",
    "Para el caso de algoritmos de **boosting** es interesante utilizar clasificadores 'weak learners' con un poder de generalizacion y parametros no muy complejos ni sobreajustados. Así se le ofrece al algoritmo de **boosting** espacio para optimizar dichos parametros y teóricamente transformar al clasificador base en un strong learner. Por ejemplo, arboles de decision muy poco profundos (shallow trees).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filtrado <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean perimeter' 'mean area' 'mean compactness'\n",
      " 'mean concavity' 'mean concave points' 'radius error' 'perimeter error'\n",
      " 'area error' 'worst radius' 'worst perimeter' 'worst area'\n",
      " 'worst compactness' 'worst concavity' 'worst concave points']\n"
     ]
    }
   ],
   "source": [
    "fss = SelectKBest(mutual_info_classif, k=int(round(dssh['wisconsin']['train']['atts'].shape[1]/2))).fit(dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'])\n",
    "\n",
    "# We can obtain the mask of the selected attributes\n",
    "print(dss['wisconsin'].feature_names[fss.get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAtts(atts, label, feature_names = None, cls = mutual_info_classif, k = 10):\n",
    "    if (not ('chi2' in str(cls)) or not (atts < 0).any()):\n",
    "        fss = SelectKBest(cls, k=k).fit(atts, label)\n",
    "        # We can obtain the mask of the selected attributes\n",
    "        fn = feature_names\n",
    "        if (str(type(feature_names)) == \"<class 'list'>\"):\n",
    "            fn = np.array(feature_names)\n",
    "        print(fn[fss.get_support()])\n",
    "        # We can transform the dataset to project only the selected attributes\n",
    "        print(fss.transform(atts).shape[1])\n",
    "        return fss\n",
    "    else:\n",
    "        print (\"chi2 doesn't work with negative values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wisconsin:\n",
      "['mean radius' 'mean perimeter' 'mean area' 'mean compactness'\n",
      " 'mean concavity' 'mean concave points' 'radius error' 'perimeter error'\n",
      " 'area error' 'worst radius' 'worst perimeter' 'worst area'\n",
      " 'worst compactness' 'worst concavity' 'worst concave points']\n",
      "15\n",
      "\n",
      "pima:\n",
      "['sex' 'bmi' 's3' 's4' 's5']\n",
      "5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredAtts = {}\n",
    "for i in dssh:\n",
    "    print(i + \":\")\n",
    "    numVar = int(round(dssh[i]['train']['atts'].shape[1]/2))\n",
    "    filteredAtts[i] = filterAtts(dssh[i]['train']['atts'],\n",
    "                                 dssh[i]['train']['label'],\n",
    "                                 feature_names=dss[i].feature_names,\n",
    "                                 k = numVar)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wisconsin:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area' 'mean concavity'\n",
      " 'radius error' 'perimeter error' 'area error' 'worst radius'\n",
      " 'worst texture' 'worst perimeter' 'worst area' 'worst compactness'\n",
      " 'worst concavity' 'worst concave points']\n",
      "15\n",
      "\n",
      "pima:\n",
      "chi2 doesn't work with negative values!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "for i in dssh:\n",
    "    print(i + \":\")\n",
    "    numVar = int(round(dssh[i]['train']['atts'].shape[1]/2))\n",
    "    filteredAtts[i] = filterAtts(dssh[i]['train']['atts'],\n",
    "                                 dssh[i]['train']['label'],\n",
    "                                 feature_names=dss[i].feature_names,\n",
    "                                 k=numVar,\n",
    "                                 cls=chi2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wrapper <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourWrapperAux(cls, atts, label, mask, checked):\n",
    "    maskL = []\n",
    "    for i in range(len(mask)):\n",
    "        if(mask[i]):\n",
    "            maskaux = mask[:]\n",
    "            maskaux[i] = False\n",
    "            if (maskL not in checked):\n",
    "                maskL.append(maskaux)\n",
    "                checked.append(maskL)\n",
    "    \n",
    "    scores = []\n",
    "    scores.append((cross_val_score(cls, atts[:,mask], label).mean(), mask))\n",
    "    for m in maskL:\n",
    "        if (any(m)):\n",
    "            scores.append(ourWrapperAux(cls, atts, label, m, checked))\n",
    "    return max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourWrapper(cls, atts, label):\n",
    "    mask = [True for i in range(0,atts.shape[1])]\n",
    "    check = []\n",
    "    res = ourWrapperAux(cls, atts, label, mask, check)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worst area' 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Score: 0.934272300469\n"
     ]
    }
   ],
   "source": [
    "clas = tree.DecisionTreeClassifier(random_state = seed)\n",
    "result = ourWrapper(clas, dssh['wisconsin']['train']['atts'], dssh['wisconsin']['train']['label'])\n",
    "print(dss['wisconsin'].feature_names[result[1]])\n",
    "print('Score: ' + str(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algoritmo de selección de parámetros voraz <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar un esquema voraz debemos tener en cuenta tres funciones que hay que especificar:\n",
    "\n",
    "* seleccionar: Permite determinar que elemento de C seleccionar en cada paso del algoritmo.\n",
    "* es prometedora: Determina si una solucion x puede extenderse para formar una solucion factible.\n",
    "* es factible: Determina si una solucion x es factible, es decir, si se ajusta a las restricciones del problema.\n",
    "\n",
    "Para poder realizarlas lo primero que se debe hacer es determinar como se va a codificar la solución, en este caso utilizamos una máscara que indica si un atributo está incluido o no en la solución.\n",
    "\n",
    "Lo primero que se realiza es un ranking de las variables en función de la información mutua, y posteriormente, comenzando por las de mayor peso son evaluadas. Si mejora se añade a la solución, en caso contrario esa variable y todas las que estén por debajo quedan descartadas y termina la ejecución del algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# We can compute the scores and ranking directly:\n",
    "\n",
    "def ranking(atts, label, feature_names = None, cls = mutual_info_classif, \n",
    "            random_state = None):\n",
    "    if ('mutual_info_classif' in str(cls)):\n",
    "        scores = list(cls(atts, label, random_state=random_state))\n",
    "    else:\n",
    "        scores = list(cls(atts, label))\n",
    "    if (len(scores) == 2 and atts.shape[1] != 2):\n",
    "        scores = scores[0] # En sklearn hay varias clasificaciones que devuelven\n",
    "        # no solo los scores, sino un array de p-valores.\n",
    "    names = list(feature_names)\n",
    "    ranks = sorted( list(zip(scores, names)), reverse=True )\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.49086511932726973, 'worst perimeter'),\n",
       " (0.46735131033834398, 'worst radius'),\n",
       " (0.46129259897582275, 'worst area'),\n",
       " (0.45061762610495726, 'worst concave points'),\n",
       " (0.43925445869247559, 'mean concave points'),\n",
       " (0.42046993708407365, 'mean perimeter'),\n",
       " (0.3817026110091668, 'mean radius'),\n",
       " (0.38042074415854388, 'mean concavity'),\n",
       " (0.37953898514885154, 'mean area'),\n",
       " (0.35996521750694987, 'area error'),\n",
       " (0.33984926646691815, 'worst concavity'),\n",
       " (0.27155716527429252, 'perimeter error'),\n",
       " (0.25919475821668869, 'radius error'),\n",
       " (0.2383517022850099, 'worst compactness'),\n",
       " (0.21122035875400647, 'mean compactness'),\n",
       " (0.16887223636501547, 'worst texture'),\n",
       " (0.15974418900132203, 'concave points error'),\n",
       " (0.1363522310238785, 'mean texture'),\n",
       " (0.12480012241200922, 'concavity error'),\n",
       " (0.092197899731085275, 'worst symmetry'),\n",
       " (0.091346535996676748, 'mean smoothness'),\n",
       " (0.079550583497442373, 'worst smoothness'),\n",
       " (0.070737887481229444, 'worst fractal dimension'),\n",
       " (0.070432782973200547, 'mean symmetry'),\n",
       " (0.069814848185615785, 'compactness error'),\n",
       " (0.052015120428781758, 'fractal dimension error'),\n",
       " (0.034386867127606635, 'smoothness error'),\n",
       " (0.0029346877322149467, 'symmetry error'),\n",
       " (0.0, 'texture error'),\n",
       " (0.0, 'mean fractal dimension')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prueba ranking\n",
    "ranking(dssh['wisconsin']['train']['atts'],\n",
    "        dssh['wisconsin']['train']['label'],\n",
    "       feature_names = dss['wisconsin'].feature_names,\n",
    "       cls=mutual_info_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedySelect(atts, label, feature_names = None, cls = None,\n",
    "                 random_state = None, scr = mutual_info_classif):\n",
    "    rank = ranking(atts, label, feature_names = feature_names, cls = scr, random_state = random_state)\n",
    "    mask = [False for i in range(atts.shape[1])]\n",
    "    score = 0\n",
    "    fnames = np.array(feature_names)\n",
    "    for i in range(atts.shape[1]):\n",
    "        nm = mask[:]\n",
    "        fname = rank[i][1]\n",
    "        index = np.where(fnames==fname)[0][0]\n",
    "        nm[index] = True\n",
    "        ns = cross_val_score(cls, atts[:,nm], label).mean()\n",
    "        if (ns > score):\n",
    "            score = ns\n",
    "            mask = nm\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = {}\n",
    "for i in dssh:\n",
    "    m[i] = greedySelect(dssh[i]['train']['atts'],\n",
    "                     dssh[i]['train']['label'],\n",
    "                     feature_names = dss[i].feature_names,\n",
    "                     scr=mutual_info_classif, \n",
    "                     cls=tree.DecisionTreeClassifier(random_state=seed),\n",
    "                     random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['worst perimeter', 'worst area', 'worst concave points'],\n",
       "      dtype='<U23')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dss['wisconsin'].feature_names[m['wisconsin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sex'],\n",
       "      dtype='<U3')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dss['pima'].feature_names)[m['pima']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
