{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2 -  Clasificación supervisada en scikit-learn\n",
    "\n",
    "## Mineria de Datos 2017/2018\n",
    "\n",
    "* [**Hernan Indibil de La Cruz Calvo**](https://github.com/Mowstyl)\n",
    "* [**Alejandro Martin Simon Sanchez**](https://github.com/elssbbboy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indice\n",
    "1. [Introducción](##Introduccion)\n",
    "2. [Clasificadores y metodos de evaluacion](##2.SeleccionModelos)\n",
    "    \n",
    "    2.1 [Uso del algoritmo Grid Search](###2.1Uso del algoritmo Grid Search)\n",
    "        2.1.1 [Tratamiento de valores perdidos](####2.1.1 Tratamiento de valores perdidos)\n",
    "        2.1.2 [Clasificador KNN](####2.1.2 Clasificador KNN)\n",
    "        2.1.3 [Árboles de decisión](####2.1.3 Árboles de decisión)\n",
    "        2.1.4 [Métricas](####2.1.4 Métricas)\n",
    "        2.1.5 [Validación cruzada](####2.1.5 Validación cruzada)\n",
    "        2.1.6 [Automatización del proceso y análisis de Pima y Wisconsin](####2.1.6 Automatización del proceso y análisis de Pima y Wisconsin)\n",
    "    2.2 [Comparación de resultados](### 2.2 Comparación de resultados)\n",
    "        2.2.1 [Pima](####2.2.1 Pima)\n",
    "        2.2.2 [Wisconsin](####2.2.2 Wisconsin)\n",
    "    2.3 [Estudio de los algoritmos](###2.3 Estudio de los algoritmos)\n",
    "        2.3.1 [Pima](####2.3.1 Pima)\n",
    "            2.3.1.1[KNN](#####2.3.1.1 KNN)\n",
    "            2.3.1.2[Tree](#####2.3.2.1 Tree)\n",
    "        2.3.2 [Wisconsin](####2.3.2 Wisconsin)\n",
    "            2.3.2.1[KNN](#####2.3.2.1 KNN)\n",
    "\n",
    "## 1. Introduccion\n",
    "Esta práctica tendrá dos partes:\n",
    "\n",
    "Primero estudiaremos la API de algunos de los clasificadores más utilizados en `scikit-learn` para conocer los distintos hiperparámetros que los configuran y estudiar los modelos resultantes.\n",
    "\n",
    "Segundo estudiaremos métodos de selección de modelos, orientados a obtener una configuración óptima de los hiperparámetros para nuestros clasificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always load all scipy stack packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, integrate\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code configures matplotlib for proper rendering\n",
    "%matplotlib inline\n",
    "mpl.rcParams[\"figure.figsize\"] = \"8, 4\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se establece una semilla predeterminada para que los experimentos sean reproducibles\n",
    "seed = 6470\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Lo siguiente es cargar los datos que se van a utilizar.**\n",
    "\n",
    "    Se usa como label la variable categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de nombre: fichero, con los datos de los dataframe a cargar\n",
    "files = {\n",
    "    'pima': '../data/pima.csv',\n",
    "    'wisconsin': '../data/wisconsin.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los dataframes\n",
    "dfs = {name: pd.read_csv(file, dtype={ \"label\": 'category'}) for name, file in files.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en la práctica anterior, las variables de Pima \"plas\", \"pres\", \"skin\", \"insu\" y \"mass\" tienen los valores perdidos codificados como 0, ya que es imposible que una persona viva tenga valor 0 en cualquiera de ellas.\n",
    "Podemos cambiar los 0 por NaN en el dataframe original sin perder información ni sobreajustar de ninguna forma, ya que no usamos información del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['pima']['plas'] = dfs['pima']['plas'].replace(0, np.nan)\n",
    "dfs['pima']['pres'] = dfs['pima']['pres'].replace(0, np.nan)\n",
    "dfs['pima']['skin'] = dfs['pima']['skin'].replace(0, np.nan)\n",
    "dfs['pima']['insu'] = dfs['pima']['insu'].replace(0, np.nan)\n",
    "dfs['pima']['mass'] = dfs['pima']['mass'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el dataframe Wisconsin la variable Patient ID no aporta nada beneficioso al proceso de clasificación, sólo sobreajuste. No tiene nada que ver con la variable clase. Por ello, procedemos a eliminarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['wisconsin'] = dfs['wisconsin'].drop('patientId', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procedemos a actualizar el diccionario de dataframes para que tenga la siguiente estructura:\n",
    "\n",
    "* dfs\n",
    "    * pima\n",
    "        * train\n",
    "            * atts\n",
    "            * label\n",
    "        * test\n",
    "            * atts\n",
    "            * label\n",
    "    * wisconsin\n",
    "        * train\n",
    "            * atts\n",
    "            * label\n",
    "        * test\n",
    "            * atts\n",
    "            * label\n",
    "\n",
    "La función utilizada no solo crea la estructura para los dataframes pima y wisconsin, sino para todos los dataframes que haya en el diccionario que se le pase. Realiza el proceso de holdout también para todos los dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def holdout(dframe, seed = None, tsize = 0.2):\n",
    "    dfAttributes = dframe.drop('label', 1)\n",
    "    dfLabel = dframe['label']\n",
    "\n",
    "    df = {}\n",
    "    df['train'] = {}\n",
    "    df['test'] = {}\n",
    "\n",
    "    df['train']['atts'], df['test']['atts'], df['train']['label'], df['test']['label'] = train_test_split(\n",
    "        dfAttributes,\n",
    "        dfLabel,\n",
    "        test_size = tsize,\n",
    "        random_state = seed,\n",
    "        stratify = dfLabel)\n",
    "\n",
    "    return df\n",
    "\n",
    "dfsh = { name: holdout(dframe, seed, 0.2) for name, dframe in dfs.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selección de modelos <a name=\"##2.SeleccionModelos\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Uso del algoritmo Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se utiliza el algoritmo Grid Search para encontrar la configuración óptima para los distintos estimadores. Dicho algoritmo recibe como parámetros el estimador a configurar y los  a ajustar con una lista de los posibles valores que puedan tomar.\n",
    "Por ello, para poder utilizarlo debemos primero ver cómo se crean los estimadores y qué variables pueden tener y en qué rango deben ser ajustadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Tratamiento de valores perdidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre los distintos dataframes es posible realizar un tratamiento de los valores perdidos.\n",
    "Es posible realizarlo mediante un Imputer de scikit, que realizará automáticamente el cambio de los mismos por la media, mediana o moda según cómo se indique la estrategia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.884365</td>\n",
       "      <td>122.502463</td>\n",
       "      <td>72.519591</td>\n",
       "      <td>29.392111</td>\n",
       "      <td>156.279874</td>\n",
       "      <td>32.596053</td>\n",
       "      <td>0.480606</td>\n",
       "      <td>33.285016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.396762</td>\n",
       "      <td>30.833718</td>\n",
       "      <td>11.955500</td>\n",
       "      <td>8.914926</td>\n",
       "      <td>87.090249</td>\n",
       "      <td>7.009831</td>\n",
       "      <td>0.331490</td>\n",
       "      <td>11.698435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>120.500000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>72.519591</td>\n",
       "      <td>29.392111</td>\n",
       "      <td>156.279874</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>156.279874</td>\n",
       "      <td>36.875000</td>\n",
       "      <td>0.639250</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.329000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             preg        plas        pres        skin        insu        mass  \\\n",
       "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
       "mean     3.884365  122.502463   72.519591   29.392111  156.279874   32.596053   \n",
       "std      3.396762   30.833718   11.955500    8.914926   87.090249    7.009831   \n",
       "min      0.000000   44.000000   24.000000    7.000000   15.000000   18.200000   \n",
       "25%      1.000000  100.000000   64.000000   25.250000  120.500000   27.500000   \n",
       "50%      3.000000  119.000000   72.519591   29.392111  156.279874   32.400000   \n",
       "75%      6.000000  142.000000   80.000000   33.000000  156.279874   36.875000   \n",
       "max     17.000000  198.000000  114.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "             pedi         age  \n",
       "count  614.000000  614.000000  \n",
       "mean     0.480606   33.285016  \n",
       "std      0.331490   11.698435  \n",
       "min      0.078000   21.000000  \n",
       "25%      0.248000   24.000000  \n",
       "50%      0.384000   29.000000  \n",
       "75%      0.639250   40.000000  \n",
       "max      2.329000   81.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo\n",
    "\n",
    "# El siguiente imputer sustituye los np.nan por la media (Valores por defecto)\n",
    "# Primero definimos el modelo\n",
    "imp = Imputer()\n",
    "\n",
    "# Ahora lo entrenamos\n",
    "imp = imp.fit(dfsh['pima']['train']['atts'])\n",
    "\n",
    "# Finalmente lo podemos usar para transformar el dataframe de la siguiente forma\n",
    "X = imp.transform(dfsh['pima']['train']['atts'])\n",
    "Y = imp.transform(dfsh['pima']['test']['atts'])\n",
    "\n",
    "# El resultado es una matriz, por lo que debemos transformarla de nuevo en un dataframe\n",
    "train_attsFull = pd.DataFrame(X, columns = dfsh['pima']['train']['atts'].columns)\n",
    "test_attsFull = pd.DataFrame(Y, columns = dfsh['pima']['test']['atts'].columns)\n",
    "\n",
    "train_attsFull.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma tenemos que por parte del Imputer las variables a configurar son:\n",
    "* missing_values: variable con el valor con el que se codifican los valores perdidos. Por defecto missing_values = 'NaN', que sustituye los np.nan.\n",
    "* strategy: variable que indica con qué se reemplazan los valores perdidos. Puede ser: 'mean', 'median' y 'most_frequent'. Por defecto strategy = 'mean'.\n",
    "* axis: eje en el que se hace la imputación (0 para columnas, 1 para filas). Por defecto axis = 0.\n",
    "\n",
    "La información ha sido obtenida de la documentación encontrada en [sklearn.preprocessing.Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html#sklearn.preprocessing.Imputer.fit_transform)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez terminado el tratamiento de valores perdidos, podemos proceder a crear los clasificadores. En esta práctica se utilizarán dos métodos de clasificación: KNN y árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Clasificador KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN es un clasificador sencillo de entender y configurar, ya que solo tendremos que fijar el parámetro `k` que determina el número de vecinos con los que compararemos.\n",
    "\n",
    "Se trata de un algoritmo perezoso, es decir, que no realiza fase de aprendizaje previa porque computa los parámetros necesarios para la clasificación durante el propio proceso de clasificación. Aunque esto pueda parecer una ventaja puede llegar a resultar ineficiente para bases de datos con muchas instancias. Además, es muy sensible a cambios en los datos de training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "\n",
    "# Se prueba el clasificador con un valor de k = 5 vecinos sin pesar por la distancia (Por defecto).\n",
    "# La distancia usada es la de Minkowski\n",
    "model = neighbors.KNeighborsClassifier()\n",
    "# Como es perezoso solo se inicializa su estado.\n",
    "\n",
    "# A continuacion se aprenden los datos del conjunto de training.\n",
    "knn = model.fit(train_attsFull, dfsh['pima']['train']['label'])\n",
    "\n",
    "predictionKNN = knn.predict(test_attsFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma tenemos que por parte del clasificador KNN los hiperparámetros a configurar son:\n",
    "* n_neighbors: variable con el número de vecinos usados. Por defecto n_neighbors = 5. Es la K de KNN.\n",
    "* weights: variable que indica cómo pesar los vecinos a la hora de clasificar, pudiendo elegir entre que todos pesen lo mismo ('uniform') o pesar por la inversa de la distancia ('distance'). También es posible pasar por parámetro una función que devuelva el peso recibiendo como argumento un array de distancias. Por defecto weights = 'uniform'.\n",
    "* metric: métrica de distancia a utilizar. Las posibles métricas pueden encontrarse en [class DistanceMetric](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html). Por defecto metric = 'minkowski'.\n",
    "* p: parámetro para la métrica de distancia 'minkowski', donde la distancia entre x e y se calcula como sum(|x - y|^p)^(1/p). Por ejemplo para p = 1 es la distancia de Manhattan y con p = 2 es la Euclídea. Por defecto p = 2.\n",
    "* metric_params: otros argumentos que pueden ser usados en la función de distancia escogida. Por ejemplo en la distancia de Mahalanobis o en la de WMinkowski.\n",
    "\n",
    "Hay más variables que sirven para mejorar la eficiencia pero que no afectan al resultado de la clasificación, que afectan por ejemplo al nivel de concurrencia. Por ello, no las trataremos de momento.\n",
    "\n",
    "La información ha sido obtenida de la documentación encontrada en [sklearn.neighbors.KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica anterior ya se trabajó con árboles de decisión, por lo que ahora nos centraremos más en los hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "\n",
    "# Iniciamos el modelo usando la impureza Gini como criterio para las divisiones, sin profundidad máxima.\n",
    "model = tree.DecisionTreeClassifier(random_state = seed)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "classifier = model.fit(train_attsFull, dfsh['pima']['train']['label'])\n",
    "\n",
    "# Obtenemos la predicción\n",
    "predictionTree = classifier.predict(test_attsFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma tenemos que por parte del árbol de decisión los hiperparámetros a configurar son:\n",
    "* criterion: variable con el criterio para hacer las divisiones. Puede ser 'gini' para usar la impureza Gini o 'entropy' para usar la ganancia de información. Por defecto criterion = 'gini'.\n",
    "* splitter: variable que permite decidir entre buscar el mejor corte con 'best' o el mejor corte aleatorio con 'random'. Por defecto splitter = 'best'.\n",
    "* max_depth: variable que establece la profundidad máxima del árbol. Reducirla puede ayudar a evitar el sobreajuste. Por defecto max_depth = None.\n",
    "* min_samples_split: variable que permite determinar el mínimo número de instancias necesarias para dividir un nodo interno. Aumentar el número permite disminuir el sobreajuste. Por defecto min_samples_split = 2.\n",
    "* min_samples_leaf: variable que permite determinar el mínimo número de casos necesarios para que un nodo sea hoja. Aumentar el número permite disminuir el sobreajuste. Por defecto min_samples_leaf = 1.\n",
    "* min_weight_fraction_leaf: se usa cuando a la hora de entrenar se han especificado pesos para los distintos casos. De no especificarse todos los casos tendrán el mismo peso. La variable indica la minima fracción de peso del total de pesos necesaria para que un nodo pueda ser hoja. Por defecto min_weight_fraction_leaf = 0. (debe ser float).\n",
    "* max_features: el número de variables a considerar a la hora de buscar el mejor corte. Se puede especificar un entero, un flotante con el porcentaje, 'auto' o 'sqrt' para tomar la raíz del número total de variables (¿por qué auto hace siempre lo mismo?), 'log2' para tomar el logaritmo en base 2 o None para tomar max_features = numero total de variables. Por defecto max_features = None.\n",
    "* max_leaf_nodes: máximo número de nodos hoja, donde el árbol se construirá con primero-mejor pesando con la impureza. Por defecto max_leaf_nodes = None.\n",
    "* min_impurity_decrease: un nodo se divide si al hacerlo se produce una disminución de impureza mayor o igual a éste valor. Aumentar el valor disminuye el sobreajuste. Por defecto min_impurity_decrease = 0.\n",
    "* class_weight: indica el peso de cada etiqueta posible de la variable clase (también puede usarse en problemas multiclase). Puede pasarse un diccionario etiqueta: peso, una lista de diccionarios, 'balanced' para que pese las etiquetas en función de la inversa de sus frecuencias de aparición o None para que todas tengan el mismo peso. Por defecto class_weight = None.\n",
    "\n",
    "Hay más hiperparámetros que sirven para mejorar la eficiencia pero que no afectan al resultado de la clasificación, como presort. También tenemos el hiperparámetro random_state, para introducir la semilla. Por ello, no las trataremos de momento.\n",
    "\n",
    "La información ha sido obtenida de la documentación encontrada en [sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evaluar un modelo es necesario utilizar métricas. Aparte de la tasa de aciertos (accuracy) es importante observar otros parámetros como la precisión, recall, f1 score y el área bajo la curva (AUC).\n",
    "Scikit-learn automatiza el proceso de obtener todas estas métricas mediante el paquete sklearn.metrics. Las métricas calculadas por sklearn son las que aparecen en la documentación [Docs: Evaluación de modelos](http://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusion - arbol: \n",
      "[[76 24]\n",
      " [25 29]]\n",
      "Matriz de confusion - KNN: \n",
      "[[88 12]\n",
      " [28 26]]\n",
      "\n",
      "Tasa de acierto - arbol: \n",
      "0.681818181818\n",
      "Tasa de acierto - KNN: \n",
      "0.74025974026\n",
      "\n",
      "Recall - arbol:\n",
      "0.537037037037\n",
      "Recall - KNN:\n",
      "0.481481481481\n",
      "\n",
      " Precision - arbol:\n",
      "0.547169811321\n",
      "Precision - arbol:\n",
      "0.684210526316\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "\n",
    "# Matriz de confusion\n",
    "print ('Matriz de confusion - arbol: ')\n",
    "print (metrics.confusion_matrix(dfsh['pima']['test']['label'], predictionTree))\n",
    "print ('Matriz de confusion - KNN: ')\n",
    "print (metrics.confusion_matrix(dfsh['pima']['test']['label'], predictionKNN))\n",
    "\n",
    "# Tasa de acierto - Accuracy\n",
    "print ('\\nTasa de acierto - arbol: ')\n",
    "print (metrics.accuracy_score(dfsh['pima']['test']['label'], predictionTree))\n",
    "\n",
    "print ('Tasa de acierto - KNN: ')\n",
    "print (metrics.accuracy_score(dfsh['pima']['test']['label'], predictionKNN))\n",
    "\n",
    "# Recall\n",
    "print('\\nRecall - arbol:')\n",
    "print(metrics.recall_score(dfsh['pima']['test']['label'], predictionTree, pos_label=\"tested_positive\"))\n",
    "print('Recall - KNN:')\n",
    "print(metrics.recall_score(dfsh['pima']['test']['label'], predictionKNN, pos_label=\"tested_positive\"))\n",
    "\n",
    "# Precision\n",
    "print('\\n Precision - arbol:')\n",
    "print(metrics.precision_score(dfsh['pima']['test']['label'], predictionTree, pos_label=\"tested_positive\"))\n",
    "print('Precision - arbol:')\n",
    "print(metrics.precision_score(dfsh['pima']['test']['label'], predictionKNN, pos_label=\"tested_positive\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5 Validación cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como se indica en [sklearn.model_selection.GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), GridSearchCV realiza un proceso de validación cruzada para obtener los mejores parámetros para los estimadores a evaluar.\n",
    "\n",
    "Para realizar la validación cruzada y obtener métricas para compararlas usamos la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70 (+/- 0.14)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "\n",
    "model = tree.DecisionTreeClassifier(random_state = seed)\n",
    "\n",
    "scores = cross_val_score(estimator=model, X=train_attsFull, y=dfsh['pima']['train']['label'], scoring=\"accuracy\", cv=10)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6 Automatización del proceso y análisis de Pima y Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación agruparemos el Imputer y el clasificador mediante el uso de Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68181818181818177"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo\n",
    "\n",
    "# Primero definimos el estimador. Este pipeline sustituye np.nan por la media y aplica el arbol de decision.\n",
    "estimator = Pipeline([(\"imputer\", Imputer()),\n",
    "                      (\"tree\", tree.DecisionTreeClassifier(random_state = seed))])\n",
    "\n",
    "# Ahora entrenamos el estimador\n",
    "cls = estimator.fit(dfsh['pima']['train']['atts'], dfsh['pima']['train']['label'])\n",
    "\n",
    "# Y podemos proceder a realizar una prediccion\n",
    "prediction = estimator.predict(dfsh['pima']['test']['atts'])\n",
    "metrics.accuracy_score(dfsh['pima']['test']['label'], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos, una vez decidido cómo se define el estimador a utilizar, a utilizar GridSearchCV para ajustar los hiperparámetros.\n",
    "\n",
    "De esta forma tenemos dos posibles casos, ambos con una parte común:\n",
    "* Estimador con Imputer y KNN\n",
    "* Estimador con Imputer y árbol de decisión\n",
    "\n",
    "Los hiperparámetros que se deben considerar al usar GridSearchCV son los explicados en los apartados dedicados a Imputer, KNN y DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputerParams = {\n",
    "    'imputer__strategy': ['median', 'mean', 'most_frequent']\n",
    "}\n",
    "\n",
    "knnParams = {\n",
    "    'knn__n_neighbors': [i for i in range(3,8)],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2]\n",
    "}\n",
    "\n",
    "treeParams = {\n",
    "    'tree__criterion': ['gini', 'entropy'],\n",
    "    'tree__splitter': ['best', 'random'],\n",
    "    'tree__max_depth': [None] + [i for i in range(5, 7)],\n",
    "    'tree__min_samples_split': [i for i in range(2, 5)],\n",
    "    'tree__max_features': [None, 'log2', 'sqrt'],\n",
    "    'tree__class_weight': [None, 'balanced'],\n",
    "    'tree__random_state': [seed]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definidos los parámetros se puede aplicar el GridSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "\n",
    "# En este ejemplo, usaremos un arbol de decision. Primero definimos el estimador\n",
    "est = Pipeline([(\"imputer\", Imputer()),\n",
    "                (\"knn\", neighbors.KNeighborsClassifier(metric = 'minkowski'))])\n",
    "\n",
    "# Se crea el objeto\n",
    "np.random.seed(seed)\n",
    "clf = GridSearchCV(\n",
    "    estimator = est,\n",
    "    param_grid = { **imputerParams, **knnParams }, # Union de ambos diccionarios\n",
    "    scoring = \"accuracy\",\n",
    "    cv = 5,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# Se procede a entrenar\n",
    "fitted = clf.fit(dfsh['pima']['train']['atts'], dfsh['pima']['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer__strategy': 'median',\n",
       " 'knn__n_neighbors': 6,\n",
       " 'knn__p': 1,\n",
       " 'knn__weights': 'uniform'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez conocidos los distintos métodos a usar, procedemos a crear una función que pasado un dataframe como parámetro, usa GridSearchCV para configurar los clasificadores y los devuelve. La función usa por defecto 'accuracy' como scorer, pero puede usar otros como recall o f1 score, especificando pos_label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def autoGridClassify(train_atts, train_label, seed = None, scoring = 'accuracy', pos_label = None, cv = 5, iid=True):\n",
    "    if (scoring in ['recall', 'precision', 'f1'] and pos_label is not None):\n",
    "        if (scoring == 'recall'):\n",
    "            score = recall_score\n",
    "        elif (scoring == 'precision'):\n",
    "            score = precision_score\n",
    "        else:\n",
    "            score = f1_score\n",
    "        scorer = make_scorer(score, pos_label=pos_label)\n",
    "    else:\n",
    "        scorer = scoring\n",
    "    \n",
    "    e1 = Pipeline([(\"imputer\", Imputer()),\n",
    "                   (\"knn\", neighbors.KNeighborsClassifier())])\n",
    "    \n",
    "    e2 = Pipeline([(\"imputer\", Imputer()),\n",
    "                   (\"tree\", tree.DecisionTreeClassifier())])\n",
    "    \n",
    "    cls = {}\n",
    "    np.random.seed(seed)\n",
    "    cls['knn'] = GridSearchCV(\n",
    "                estimator = e1,\n",
    "                param_grid = { **imputerParams, **knnParams }, # Union de ambos diccionarios\n",
    "                scoring = scorer,\n",
    "                cv = cv,\n",
    "                n_jobs = -1,\n",
    "                iid = iid\n",
    "            )\n",
    "    \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    cls['tree'] = GridSearchCV(\n",
    "                estimator = e2,\n",
    "                param_grid = { **imputerParams, **treeParams }, # Union de ambos diccionarios\n",
    "                scoring = scorer,\n",
    "                cv = cv,\n",
    "                n_jobs = -1,\n",
    "                iid = iid\n",
    "            )\n",
    "    \n",
    "    fitted = {}\n",
    "    fitted['knn'] = cls['knn'].fit(train_atts, train_label)\n",
    "    fitted['tree'] = cls['tree'].fit(train_atts, train_label)\n",
    "    \n",
    "    return cls, fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente procedemos a realizar el proceso completo de clasificación con Pima y Wisconsin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recordemos que en dfsh ya estan todos los dataframes tras realizar el holdout\n",
    "cls = {}\n",
    "fitted = {}\n",
    "results = {}\n",
    "\n",
    "for k in dfsh:\n",
    "    results[k] = {}\n",
    "    if (k == 'pima'):\n",
    "        pl = 'tested_positive'\n",
    "    elif (k == 'wisconsin'):\n",
    "        pl = 'malignant'\n",
    "    else:\n",
    "        pl = None\n",
    "    \n",
    "    cls[k], fitted[k] = autoGridClassify(dfsh[k]['train']['atts'], dfsh[k]['train']['label'], seed=seed,\n",
    "                                        scoring = 'f1', pos_label = pl, cv = 10, iid = False)\n",
    "    \n",
    "    for alg, cl in cls[k].items():\n",
    "        results[k][alg] = cl.predict(dfsh[k]['test']['atts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el diccionario results tenemos los resultados de las predicciones de cada dataframe con cada algoritmo de la siguiente manera:\n",
    "\n",
    "    results['DataFrame']['Algoritmo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Comparación de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez obtenidos los clasificadores y hechas las predicciones, podemos a proceder a comparar los resultados obtenidos por cada clasificador en cada base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metodo que pasado test_label y un diccionario con predicciones indizadas\n",
    "#   por el algoritmo devuelve una tabla con las metricas utilizadas\n",
    "\n",
    "def metricTable(test_label, predictions, pos_label):\n",
    "    algL = []\n",
    "    accuracyL = []\n",
    "    recallL = []\n",
    "    precisionL = []\n",
    "    f1scoreL = []\n",
    "    aucL = []\n",
    "\n",
    "    for alg, prediction in predictions.items():\n",
    "        algL.append(alg)\n",
    "        accuracyL.append(metrics.accuracy_score(test_label, prediction))\n",
    "        recallL.append(metrics.recall_score(test_label, prediction, pos_label=pos_label))\n",
    "        precisionL.append(metrics.precision_score(test_label, prediction, pos_label=pos_label))\n",
    "        f1scoreL.append(metrics.f1_score(test_label, prediction, pos_label=pos_label))\n",
    "        aucL.append(metrics.roc_auc_score(y_true=pd.get_dummies(test_label), y_score=pd.get_dummies(prediction)))\n",
    "\n",
    "    table = [('Algorithm', algL),\n",
    "             ('Accuracy', accuracyL),\n",
    "             ('Recall', recallL),\n",
    "             ('Precision', precisionL),\n",
    "             ('F1 Score', f1scoreL),\n",
    "             ('ROC AUC', aucL)\n",
    "             ]\n",
    "\n",
    "    return pd.DataFrame.from_items(table)\n",
    "\n",
    "def cMatrix(matrix, pos_label, neg_label):\n",
    "    rowL = [pos_label, neg_label]\n",
    "    tColL = [matrix[1,1], matrix[0,1]]\n",
    "    fColL = [matrix[1,0], matrix[0,0]]\n",
    "    table = [('Actual \\ Pred', rowL),\n",
    "             (pos_label, tColL),\n",
    "             (neg_label, fColL)]\n",
    "    return pd.DataFrame.from_items(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Pima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Actual \\ Pred  tested_positive  tested_negative\n",
      "tested_positive               26               28\n",
      "tested_negative               12               88\n",
      "\n",
      "Tree\n",
      "Actual \\ Pred  tested_positive  tested_negative\n",
      "tested_positive               35               19\n",
      "tested_negative               22               78\n"
     ]
    }
   ],
   "source": [
    "print(\"KNN\")\n",
    "c = cMatrix(metrics.confusion_matrix(dfsh['pima']['test']['label'], results['pima']['knn']), 'tested_positive', 'tested_negative')\n",
    "print(c.to_string(index=False))\n",
    "print(\"\\nTree\")\n",
    "c = cMatrix(metrics.confusion_matrix(dfsh['pima']['test']['label'], results['pima']['tree']), 'tested_positive', 'tested_negative')\n",
    "print(c.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando la matriz de confusión vemos que el árbol aunque acierte más detectando positivos que KNN, lo hace a costa de un aumento de los falsos positivos. Por otro lado, esto trae consigo una reducción en los falsos negativos aunque también en los verdaderos negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.680741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.733766</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.630631</td>\n",
       "      <td>0.714074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithm  Accuracy    Recall  Precision  F1 Score   ROC AUC\n",
       "0       knn  0.740260  0.481481   0.684211  0.565217  0.680741\n",
       "1      tree  0.733766  0.648148   0.614035  0.630631  0.714074"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricTable(dfsh['pima']['test']['label'], results['pima'], 'tested_positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el accuracy y la precisión usando KNN sean ligeramente mayores, en éste problema los falsos negativos pueden traer consigo un coste humano, por lo que se preferiría el árbol de decisión. Éste tiene un mayor recall y F1 score (a pesar de que tenga una precisión un poco menor), además de tener mayor AUC.\n",
    "Por ello, el modelo preferido sería el creado usando el árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Actual \\ Pred  malignant  benign\n",
      "   malignant         47       1\n",
      "      benign          3      89\n",
      "\n",
      "Tree\n",
      "Actual \\ Pred  malignant  benign\n",
      "   malignant         46       2\n",
      "      benign          6      86\n"
     ]
    }
   ],
   "source": [
    "print(\"KNN\")\n",
    "c = cMatrix(metrics.confusion_matrix(dfsh['wisconsin']['test']['label'], results['wisconsin']['knn']), 'malignant', 'benign')\n",
    "print(c.to_string(index=False))\n",
    "print(\"\\nTree\")\n",
    "c = cMatrix(metrics.confusion_matrix(dfsh['wisconsin']['test']['label'], results['wisconsin']['tree']), 'malignant', 'benign')\n",
    "print(c.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la matriz de confusión se puede observar que KNN tiene un mayor número de verdaderos positivos y verdaderos negativos que el árbol, lo que trae consigo un menor número de falsos positivos y negativos lo que a priori hace al modelo creado con KNN el mejor para éste problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>knn</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.973279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tree</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.946558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Algorithm  Accuracy    Recall  Precision  F1 Score   ROC AUC\n",
       "0       knn  0.971429  0.979167   0.940000  0.959184  0.973279\n",
       "1      tree  0.942857  0.958333   0.884615  0.920000  0.946558"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricTable(dfsh['wisconsin']['test']['label'], results['wisconsin'], 'malignant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el dataframe Wisconsin, KNN obtiene un mejor resultado en prácticamente todas las métricas consideradas, por lo que el mejor modelo es el generado con KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Estudio de los algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a analizar por separado los hiperparámetros obtenidos al aplicar GridSearchCV, viendo las fronteras de decisión generadas en cada uno de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print boundaries\n",
    "def classifierPrintBoundaries(model, train_atts, train_label, test_atts, test_label):\n",
    "\n",
    "    attsPair = [ (x,y) for x in train_atts.columns for y in train_atts.columns if x != y]\n",
    "    \n",
    "    for (att1_name, att2_name) in attsPair:\n",
    "        \n",
    "        xx, yy = np.meshgrid(np.arange(min(train_atts[att1_name])-1, max(train_atts[att1_name])+1, 0.05),\n",
    "                             np.arange(min(train_atts[att2_name])-1, max(train_atts[att2_name])+1, 0.05))\n",
    "\n",
    "        mesh = pd.DataFrame({ 'x' : xx.ravel(), 'y' : yy.ravel() })\n",
    "        \n",
    "        cls = model.fit(train_atts[[att1_name, att2_name]], train_label)\n",
    "\n",
    "        Z = cls.predict(mesh)\n",
    "        mesh = mesh.assign( label = pd.Categorical(Z, categories=train_label.cat.categories) )\n",
    "\n",
    "        colors = [\"#4D73AB\",\"#54A86F\",\"#C44D54\"]\n",
    "\n",
    "        mesh = mesh.assign(colors = mesh.label.cat.codes.map(lambda x: colors[x]))\n",
    "        colorBoundary = list(mesh.label.cat.codes.map(lambda x: colors[x]))\n",
    "        colorObservations = list(test_label.cat.codes.map(lambda x: colors[x]))\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        # Plot using Seaborn\n",
    "        sns.regplot(x='x', y='y', data=mesh,\n",
    "                   fit_reg=False, \n",
    "                   scatter_kws={'color': colorBoundary})\n",
    "\n",
    "        sns.regplot(x=att1_name, y=att2_name, data=test_atts,\n",
    "                   fit_reg=False,\n",
    "                   scatter_kws={'color': colorObservations,  'lw': 1, 'edgecolor':'#FFFFFF'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a tree\n",
    "import pydotplus \n",
    "from IPython.display import Image  \n",
    "def imprimeArbol(model, data, width):\n",
    "    fnames = data.columns.values[:-1]\n",
    "    lnames = data['label'].unique()\n",
    "    dot_data = tree.export_graphviz(model, out_file=None, \n",
    "                             feature_names=fnames,  \n",
    "                             class_names=lnames,  \n",
    "                             filled=True, rounded=True,  \n",
    "                             special_characters=True)  \n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "    return Image(graph.create_png(), width=width)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Pima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1.1 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer__strategy': 'median',\n",
       " 'knn__n_neighbors': 7,\n",
       " 'knn__p': 1,\n",
       " 'knn__weights': 'uniform'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best configuration\n",
    "fitted['pima']['knn'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586 (+/-0.223) for {'imputer__strategy': 'median', 'knn__n_neighbors': 7, 'knn__p': 1, 'knn__weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "best = {}\n",
    "best['pima'] = {}\n",
    "\n",
    "# Get the mean score for each cv\n",
    "means = fitted['pima']['knn'].cv_results_['mean_test_score']\n",
    "# Get the sd score for each cv\n",
    "stds = fitted['pima']['knn'].cv_results_['std_test_score']\n",
    "# Get each specific configuration\n",
    "conf = fitted['pima']['knn'].cv_results_['params']\n",
    "\n",
    "# Save the three things togheter\n",
    "for mean, std, params in zip(means, stds, conf):\n",
    "    if (params == fitted['pima']['knn'].best_params_):\n",
    "        best['pima']['knn'] = (mean, std * 2, params)\n",
    "        \n",
    "print (\"%0.3f (+/-%0.03f) for %r\" % best['pima']['knn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto a KNN aplicado en Pima, tenemos que la configuración cuyo modelo generado tiene (en media al aplicar la validación cruzada) un mayor recall tiene los siguientes hiperparámetros:\n",
    "* Número de vecinos: 7\n",
    "* Fórmula distancia: Manhattan\n",
    "* Peso de los vecinos: Uniforme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un bajo número de vecinos haría al clasificador muy sensible al ruido y por lo tanto tendría mucha varianza. La frontera de decisión se verá muy afectada por los outliers y no estará muy bien definida (overfitting).\n",
    "Por otro lado, si la k es muy alta aunque baje la varianza el sesgo aumenta ya que podría no ser capaz de capturar ciertas zonas del espacio al tratarlas como si fueran ruido, generaliza demasiado (underfitting).\n",
    "El número de vecinos escogido es el que minimiza sesgo y varianza, es decir, con un número menor de vecinos se verá afectado por el ruido a la hora de clasificar y con un mayor número no será capaz de clasificar bien ciertos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que el peso de los vecinos sea uniforme quiere decir que a la hora de aplicar el algoritmo sólo nos importan los k vecinos más cercanos y no lo cerca que están del caso a clasificar dichos vecinos, no ponderamos por distancia.\n",
    "La fórmula utilizada para el cálculo de la distancia en éste caso de Manhattan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.2 Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer__strategy': 'median',\n",
       " 'tree__class_weight': 'balanced',\n",
       " 'tree__criterion': 'entropy',\n",
       " 'tree__max_depth': 6,\n",
       " 'tree__max_features': 'log2',\n",
       " 'tree__min_samples_split': 4,\n",
       " 'tree__random_state': 6470,\n",
       " 'tree__splitter': 'best'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best configuration\n",
    "fitted['pima']['tree'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.669 (+/-0.108) for {'imputer__strategy': 'median', 'tree__class_weight': 'balanced', 'tree__criterion': 'entropy', 'tree__max_depth': 6, 'tree__max_features': 'log2', 'tree__min_samples_split': 4, 'tree__random_state': 6470, 'tree__splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Get the mean score for each cv\n",
    "means = fitted['pima']['tree'].cv_results_['mean_test_score']\n",
    "# Get the sd score for each cv\n",
    "stds = fitted['pima']['tree'].cv_results_['std_test_score']\n",
    "# Get each specific configuration\n",
    "conf = fitted['pima']['tree'].cv_results_['params']\n",
    "\n",
    "# Save the three things togheter\n",
    "for mean, std, params in zip(means, stds, conf):\n",
    "    if (params == fitted['pima']['tree'].best_params_):\n",
    "        best['pima']['tree'] = (mean, std * 2, params)\n",
    "        \n",
    "print (\"%0.3f (+/-%0.03f) for %r\" % best['pima']['tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto a Tree en Pima, tenemos que la configuración cuyo modelo generado tiene (en media al aplicar la validación cruzada) un mayor recall tiene los siguientes hiperparámetros:\n",
    "* Peso de la clase: balanced\n",
    "* Criterio: entropy\n",
    "* Profundidad máxima: 6\n",
    "* Número máximo de variables predictoras: log2\n",
    "* Mínimo de muestras para separar: 4\n",
    "* Separador: best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el peso dado a la clase es balanceado, de forma que pesa las etiquetas en función de la inversa de sus frecuencias de aparición, por lo que el peso que se le asigna a la clase mayoritaria es menor.\n",
    "\n",
    "Se toma como criterio de clasificación la entropía, así utilizando la ganancia de información para construir el árbol.\n",
    "\n",
    "La profundidad máxima que puede tomar el árbol es de 6, no dejando que se expanda en su totalidad. Eso permite evitar sobreajuste, reduciendo así la varianza y haciendo la frontera de decisión más definida. Por otro lado, una profundidad máxima demasiado reducida provocaría underfitting, dicho de otra forma aumentaría el sesgo.\n",
    "\n",
    "El mínimo de muestras necesarias para separar actua también notablemente sobre la frontera de decisión, un mayor número reduce el sobreajuste y la varianza, pero puede llegar a aumentar el sesgo.\n",
    "\n",
    "Que como separador se tome 'best' indica que a la hora de realizar los cortes en las variables se buscará el mejor, en lugar de cortar aleatoriamente.\n",
    "\n",
    "No se toman todas las variables predictoras a la hora de construir el árbol, sino tan solo el logaritmo del número total de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Wisconsin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.1 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer__strategy': 'median',\n",
       " 'knn__n_neighbors': 7,\n",
       " 'knn__p': 2,\n",
       " 'knn__weights': 'uniform'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best configuration\n",
    "fitted['wisconsin']['knn'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.956 (+/-0.040) for {'imputer__strategy': 'median', 'knn__n_neighbors': 7, 'knn__p': 2, 'knn__weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "best['wisconsin'] = {}\n",
    "\n",
    "# Get the mean score for each cv\n",
    "means = fitted['wisconsin']['knn'].cv_results_['mean_test_score']\n",
    "# Get the sd score for each cv\n",
    "stds = fitted['wisconsin']['knn'].cv_results_['std_test_score']\n",
    "# Get each specific configuration\n",
    "conf = fitted['wisconsin']['knn'].cv_results_['params']\n",
    "\n",
    "# Save the three things togheter\n",
    "for mean, std, params in zip(means, stds, conf):\n",
    "    if (params == fitted['wisconsin']['knn'].best_params_):\n",
    "        best['wisconsin']['knn'] = (mean, std * 2, params)\n",
    "        \n",
    "print (\"%0.3f (+/-%0.03f) for %r\" % best['wisconsin']['knn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto a Wisconsin, tenemos que la configuración cuyo modelo generado tiene (en media al aplicar la validación cruzada) un mayor recall tiene los siguientes hiperparámetros:\n",
    "* Número de vecinos: 7\n",
    "* Fórmula distancia: Euclídea\n",
    "* Peso de los vecinos: Uniforme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo usar 7 como número de vecinos tiene un buen equilibrio entre sesgo y varianza igual que en Pima.\n",
    "La única diferencia respecto a lo obtenido en Pima es que utilizamos la distancia Euclídea para calcular la distancia, lo cual no tiene un efecto sobre la frontera de decisión tan grande como el número de vecinos.\n",
    "También viendo el recall obtenido podemos deducir que KNN funciona mejor en Wisconsin que en Pima.\n",
    "Un apunte importante sobre KNN es que no estamos normalizando las variables predictoras, por lo que las variables que se muevan en un rango más amplio tendrán mucho más peso que las que no lo hacen.\n",
    "Posiblemente éste hecho esté provocando que Wisconsin, donde las variables tienen un rango más reducido como se vió en el análisis exploratorio, obtenga mejores resultados al usarse KNN que en Pima, donde ciertas variables tienen un rango tan grande que hace que el resto sean casi despreciables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.2 Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer__strategy': 'mean',\n",
       " 'tree__class_weight': 'balanced',\n",
       " 'tree__criterion': 'gini',\n",
       " 'tree__max_depth': 5,\n",
       " 'tree__max_features': None,\n",
       " 'tree__min_samples_split': 4,\n",
       " 'tree__random_state': 6470,\n",
       " 'tree__splitter': 'random'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best configuration\n",
    "fitted['wisconsin']['tree'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941 (+/-0.068) for {'imputer__strategy': 'mean', 'tree__class_weight': 'balanced', 'tree__criterion': 'gini', 'tree__max_depth': 5, 'tree__max_features': None, 'tree__min_samples_split': 4, 'tree__random_state': 6470, 'tree__splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "# Get the mean score for each cv\n",
    "means = fitted['wisconsin']['tree'].cv_results_['mean_test_score']\n",
    "# Get the sd score for each cv\n",
    "stds = fitted['wisconsin']['tree'].cv_results_['std_test_score']\n",
    "# Get each specific configuration\n",
    "conf = fitted['wisconsin']['tree'].cv_results_['params']\n",
    "\n",
    "# Save the three things togheter\n",
    "for mean, std, params in zip(means, stds, conf):\n",
    "    if (params == fitted['wisconsin']['tree'].best_params_):\n",
    "        best['wisconsin']['tree'] = (mean, std * 2, params)\n",
    "        \n",
    "print (\"%0.3f (+/-%0.03f) for %r\" % best['wisconsin']['tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respecto a Tree en Pima, tenemos que la configuración cuyo modelo generado tiene (en media al aplicar la validación cruzada) un mayor recall tiene los siguientes hiperparámetros:\n",
    "* Peso de la clase: balanced\n",
    "* Criterio: gini\n",
    "* Profundidad máxima: 5\n",
    "* Número máximo de variables predictoras: None\n",
    "* Mínimo de muestras para separar: 4\n",
    "* Separador: random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el peso dado a la clase es balanceado, de forma que pesa las etiquetas en función de la inversa de sus frecuencias de aparición, por lo que el peso que se le asigna a la clase mayoritaria es menor.\n",
    "\n",
    "Se toma como criterio de clasificación gini, así utilizando la impureza para construir el árbol.\n",
    "\n",
    "La profundidad máxima que puede tomar el árbol es de 5, no dejando que se expanda en su totalidad. Eso permite evitar sobreajuste, reduciendo así la varianza y haciendo la frontera de decisión más definida. Por otro lado, una profundidad máxima demasiado reducida provocaría underfitting, dicho de otra forma aumentaría el sesgo.\n",
    "\n",
    "El mínimo de muestras necesarias para separar actua también notablemente sobre la frontera de decisión, un mayor número reduce el sobreajuste y la varianza, pero puede llegar a aumentar el sesgo.\n",
    "\n",
    "Que como separador se tome 'random' indica que a la hora de realizar los cortes se realizarán aleatoriamente.\n",
    "\n",
    "Se toman todas las variables predictoras a la hora de construir el árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementación GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer__strategy': ['median', 'mean', 'most_frequent'],\n",
       " 'knn__n_neighbors': [3, 4, 5, 6, 7],\n",
       " 'knn__p': [1, 2],\n",
       " 'knn__weights': ['uniform', 'distance']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{**imputerParams, **knnParams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer__strategy': ['median', 'mean', 'most_frequent'],\n",
       " 'tree__class_weight': [None, 'balanced'],\n",
       " 'tree__criterion': ['gini', 'entropy'],\n",
       " 'tree__max_depth': [None, 5, 6],\n",
       " 'tree__max_features': [None, 'log2', 'sqrt'],\n",
       " 'tree__min_samples_split': [2, 3, 4],\n",
       " 'tree__random_state': [6470],\n",
       " 'tree__splitter': ['best', 'random']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{**imputerParams, **treeParams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "class GridSearchCVHomeModel:\n",
    "    def __init__(self, scoring, estimator, param_grid, n_jobs, fit_params, cv, verbose, pre_dispatch):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.n_jobs = n_jobs\n",
    "        self.fit_params = fit_params\n",
    "        self.cv = cv\n",
    "        self.verbose = verbose\n",
    "        self.pre_dispatch = pre_dispatch\n",
    "    \n",
    "    def fit(self, train_atts, train_label, seed=None):\n",
    "        if type(self.param_grid) is dict:\n",
    "            self.param_grid = [self.param_grid]\n",
    "\n",
    "        self.best_score_ = 0\n",
    "        self.cv_results_ = {}\n",
    "        self.cv_results_['mean_train_score'] = np.array([])\n",
    "        self.cv_results_['mean_test_score'] = np.array([])\n",
    "        self.cv_results_['std_train_score'] = np.array([])\n",
    "        self.cv_results_['std_test_score'] = np.array([])\n",
    "        self.cv_results_['mean_fit_time'] = np.array([])\n",
    "        self.cv_results_['std_fit_time'] = np.array([])\n",
    "        self.cv_results_['mean_score_time'] = np.array([])\n",
    "        self.cv_results_['std_score_time'] = np.array([])\n",
    "        self.cv_results_['params'] = np.array([])\n",
    "        self.cv_results_['rank_test_score'] = np.array([])\n",
    "\n",
    "        for pg in self.param_grid:\n",
    "            params = {}\n",
    "            for args in itertools.product(*pg.values()):\n",
    "                for i in range(0, len(args)):\n",
    "                    params[list(pg.keys())[i]] = args[i]\n",
    "                self.estimator.set_params(**params)\n",
    "                np.random.seed(seed)\n",
    "                score = cross_validate(self.estimator, train_atts, train_label, scoring=self.scoring, cv=self.cv,\n",
    "                                        n_jobs=self.n_jobs, fit_params=self.fit_params, verbose=self.verbose,\n",
    "                                        pre_dispatch=self.pre_dispatch, return_train_score=True)\n",
    "                \n",
    "                self.cv_results_['params'] = np.append(self.cv_results_['params'], params)\n",
    "                self.cv_results_['mean_train_score'] = np.append(self.cv_results_['mean_train_score'], score['train_score'].mean())\n",
    "                self.cv_results_['mean_test_score'] = np.append(self.cv_results_['mean_test_score'], score['test_score'].mean())\n",
    "                self.cv_results_['std_train_score'] = np.append(self.cv_results_['std_train_score'], score['train_score'].std())\n",
    "                self.cv_results_['std_test_score'] = np.append(self.cv_results_['std_test_score'], score['test_score'].std())\n",
    "                self.cv_results_['mean_fit_time'] = np.append(self.cv_results_['mean_fit_time'], score['fit_time'].mean())\n",
    "                self.cv_results_['std_fit_time'] = np.append(self.cv_results_['std_fit_time'], score['fit_time'].std())\n",
    "                self.cv_results_['mean_score_time'] = np.append(self.cv_results_['mean_score_time'], score['score_time'].mean())\n",
    "                self.cv_results_['std_score_time'] = np.append(self.cv_results_['std_score_time'], score['score_time'].std())\n",
    "                \n",
    "        self.cv_results_['rank_test_score'] = np.array([i[0]+1 for i in sorted(\n",
    "            enumerate(self.cv_results_['mean_test_score']), key=lambda x:x[1])])\n",
    "        \n",
    "        self.best_index_ = np.where(self.cv_results_['rank_test_score']==1)[0][0]\n",
    "        self.best_score_ = self.cv_results_['mean_test_score'][self.best_index_]\n",
    "        self.best_params_ = self.cv_results_['params'][self.best_index_]\n",
    "\n",
    "        self.estimator.set_params(**self.best_params_)\n",
    "        self.estimator.fit(train_atts, train_label)\n",
    "        self.best_estimator_ = self.estimator\n",
    "        \n",
    "    def predict(self, test_atts):\n",
    "        return self.best_estimator_.predict(test_atts)\n",
    "\n",
    "def GridSearchCVHome(estimator, param_grid, scoring=None, cv=None,\n",
    "                    fit_params=None, n_jobs=1, verbose=0, pre_dispatch='2*n_jobs'):\n",
    "    return GridSearchCVHomeModel(scoring, estimator, param_grid, n_jobs, fit_params, cv, verbose, pre_dispatch)\n",
    "\n",
    "est = Pipeline([(\"imputer\", Imputer()),\n",
    "                   (\"knn\", neighbors.KNeighborsClassifier(n_jobs=-1))])\n",
    "\n",
    "sc = make_scorer(recall_score, pos_label='tested_positive')\n",
    "clas = GridSearchCVHome(est, {**imputerParams, **knnParams}, cv=2, n_jobs=-1, scoring=sc)\n",
    "clas.fit(dfsh['pima']['train']['atts'], dfsh['pima']['train']['label'], seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer__strategy': 'most_frequent',\n",
       " 'knn__n_neighbors': 7,\n",
       " 'knn__p': 2,\n",
       " 'knn__weights': 'distance'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clas.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51869158878504673"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clas.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSCV_HOME-KNN</td>\n",
       "      <td>0.746753</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.685741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSCV_SKL-KNN</td>\n",
       "      <td>0.740260</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.680741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Algorithm  Accuracy    Recall  Precision  F1 Score   ROC AUC\n",
       "0  GSCV_HOME-KNN  0.746753  0.481481   0.702703  0.571429  0.685741\n",
       "1   GSCV_SKL-KNN  0.740260  0.481481   0.684211  0.565217  0.680741"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricTable(dfsh['pima']['test']['label'], {'GSCV_HOME-KNN': clas.predict(dfsh['pima']['test']['atts']),\n",
    "                                           'GSCV_SKL-KNN': cls['pima']['knn'].predict(dfsh['pima']['test']['atts'])}, 'tested_positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.55371614,  0.55034796,  0.53808367,  0.53021704,  0.51174098,\n",
       "        0.55579032,  0.52402267,  0.53092709,  0.58201852,  0.56903229,\n",
       "        0.57064279,  0.56079718,  0.52227205,  0.57107881,  0.54132167,\n",
       "        0.55474244,  0.58584321,  0.57798342,  0.57602363,  0.56981431,\n",
       "        0.51661937,  0.51313505,  0.52899846,  0.51917353,  0.47972372,\n",
       "        0.53787246,  0.49633963,  0.52223489,  0.56470458,  0.55877389,\n",
       "        0.53548244,  0.5288551 ,  0.53481964,  0.57974242,  0.53699419,\n",
       "        0.55358634,  0.56448254,  0.56210414,  0.56741045,  0.56651262,\n",
       "        0.55463358,  0.55463358,  0.55934272,  0.55070132,  0.52734366,\n",
       "        0.56152563,  0.49254896,  0.54697231,  0.57812362,  0.57101614,\n",
       "        0.55933225,  0.54637614,  0.50243035,  0.57167123,  0.51702214,\n",
       "        0.54804622,  0.57526297,  0.576953  ,  0.56819199,  0.56830632])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls['pima']['knn'].cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586 (+/-0.223) for {'imputer__strategy': 'median', 'knn__n_neighbors': 7, 'knn__p': 1, 'knn__weights': 'uniform'}\n",
      "0.568 (+/-0.136) for {'imputer__strategy': 'most_frequent', 'knn__n_neighbors': 7, 'knn__p': 2, 'knn__weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# Get the mean score for each cv\n",
    "means = fitted['pima']['knn'].cv_results_['mean_test_score']\n",
    "# Get the sd score for each cv\n",
    "stds = fitted['pima']['knn'].cv_results_['std_test_score']\n",
    "# Get each specific configuration\n",
    "conf = fitted['pima']['knn'].cv_results_['params']\n",
    "\n",
    "# Save the three things togheter\n",
    "for mean, std, params in zip(means, stds, conf):\n",
    "    if (params == fitted['pima']['knn'].best_params_ or params == clas.best_params_):\n",
    "        print (\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58584320590128025"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted['pima']['knn'].best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = Pipeline([(\"imputer\", Imputer()),\n",
    "                   (\"knn\", neighbors.KNeighborsClassifier(n_jobs=-1))])\n",
    "\n",
    "score = cross_validate(temp, dfsh['pima']['train']['atts'], dfsh['pima']['train']['label'],\n",
    "                       scoring='accuracy', cv=2, n_jobs=-1, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00835299,  0.02134514])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score['fit_time']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
