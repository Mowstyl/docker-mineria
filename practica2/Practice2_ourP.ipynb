{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 2 -  Clasificación supervisada en scikit-learn\n",
    "\n",
    "## Mineria de Datos 2017/2018\n",
    "\n",
    "* **Hernan Indibil de La Cruz Calvo**\n",
    "* **Alejandro Martin Simon Sanchez**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indice\n",
    "1. Introduccion\n",
    "2. Clasificadores y metodos de evaluacion\n",
    "\n",
    "\n",
    "## 1. Introduccion\n",
    "Esta práctica tendrá dos partes:\n",
    "\n",
    "Primero estudiaremos la API de algunos de los clasificadores más utilizados en `scikit-learn` para conocer los distintos hiperparámetros que los configuran y estudiar los modelos resultantes.\n",
    "\n",
    "Segundo estudiaremos métodos de selección de modelos, orientados a obtener una configuración óptima de los hiperparámetros para nuestros clasificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always load all scipy stack packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats, integrate\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code configures matplotlib for proper rendering\n",
    "%matplotlib inline\n",
    "mpl.rcParams[\"figure.figsize\"] = \"8, 4\"\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se establece una semilla predeterminada para que los experimentos sean reproducibles\n",
    "seed = 6470\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Lo siguiente es cargar los datos que se van a utilizar.**\n",
    "\n",
    "    Se usa como label la variable categórica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de nombre: fichero, con los datos de los dataframe a cargar\n",
    "files = {\n",
    "    'pima': '../data/pima.csv',\n",
    "    'wisconsin': '../data/wisconsin.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan los dataframes\n",
    "dfs = {name: pd.read_csv(file, dtype={ \"label\": 'category'}) for name, file in files.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos en la práctica anterior, las variables de Pima \"plas\", \"pres\", \"skin\", \"insu\" y \"mass\" tienen los valores perdidos codificados como 0, ya que es imposible que una persona viva tenga valor 0 en cualquiera de ellas.\n",
    "Podemos cambiar los 0 por NaN en el dataframe original sin perder información ni sobreajustar de ninguna forma, ya que no usamos información del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['pima']['plas'] = dfs['pima']['plas'].replace(0, np.nan)\n",
    "dfs['pima']['pres'] = dfs['pima']['pres'].replace(0, np.nan)\n",
    "dfs['pima']['skin'] = dfs['pima']['skin'].replace(0, np.nan)\n",
    "dfs['pima']['insu'] = dfs['pima']['insu'].replace(0, np.nan)\n",
    "dfs['pima']['mass'] = dfs['pima']['mass'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el dataframe Wisconsin la variable Patient ID no aporta nada beneficioso al proceso de clasificación, sólo sobreajuste. No tiene nada que ver con la variable clase. Por ello, procedemos a eliminarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['wisconsin'] = dfs['wisconsin'].drop('patientId', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procedemos a actualizar el diccionario de dataframes para que tenga la siguiente estructura:\n",
    "\n",
    "* dfs\n",
    "    * pima\n",
    "        * train\n",
    "            * atts\n",
    "            * label\n",
    "        * test\n",
    "            * atts\n",
    "            * label\n",
    "    * wisconsin\n",
    "        * train\n",
    "            * atts\n",
    "            * label\n",
    "        * test\n",
    "            * atts\n",
    "            * label\n",
    "\n",
    "La función utilizada no solo crea la estructura para los dataframes pima y wisconsin, sino para todos los dataframes que haya en el diccionario que se le pase. Realiza el proceso de holdout también para todos los dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def holdout(dframe, seed, tsize = 0.2):\n",
    "    dfAttributes = dframe.drop('label', 1)\n",
    "    dfLabel = dframe['label']\n",
    "\n",
    "    df = {}\n",
    "    df['train'] = {}\n",
    "    df['test'] = {}\n",
    "\n",
    "    df['train']['atts'], df['test']['atts'], df['train']['label'], df['test']['label'] = train_test_split(\n",
    "        dfAttributes,\n",
    "        dfLabel,\n",
    "        test_size = tsize,\n",
    "        random_state = seed,\n",
    "        stratify = dfLabel)\n",
    "\n",
    "    return df\n",
    "\n",
    "dfsh = { name: holdout(dframe, seed, 0.2) for name, dframe in dfs.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selección de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Uso del algoritmo Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado se utiliza el algoritmo Grid Search para encontrar la configuración óptima para los distintos estimadores. Dicho algoritmo recibe como parámetros el estimador a configurar y los  a ajustar con una lista de los posibles valores que puedan tomar.\n",
    "Por ello, para poder utilizarlo debemos primero ver cómo se crean los estimadores y qué variables pueden tener y en qué rango deben ser ajustadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Tratamiento de valores perdidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre los distintos dataframes es posible realizar un tratamiento de los valores perdidos.\n",
    "Es posible realizarlo mediante un Imputer de scikit, que realizará automáticamente el cambio de los mismos por la media, mediana o moda según cómo se indique la estrategia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>insu</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.884365</td>\n",
       "      <td>122.502463</td>\n",
       "      <td>72.519591</td>\n",
       "      <td>29.392111</td>\n",
       "      <td>156.279874</td>\n",
       "      <td>32.596053</td>\n",
       "      <td>0.480606</td>\n",
       "      <td>33.285016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.396762</td>\n",
       "      <td>30.833718</td>\n",
       "      <td>11.955500</td>\n",
       "      <td>8.914926</td>\n",
       "      <td>87.090249</td>\n",
       "      <td>7.009831</td>\n",
       "      <td>0.331490</td>\n",
       "      <td>11.698435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>120.500000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>72.519591</td>\n",
       "      <td>29.392111</td>\n",
       "      <td>156.279874</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>156.279874</td>\n",
       "      <td>36.875000</td>\n",
       "      <td>0.639250</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.329000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             preg        plas        pres        skin        insu        mass  \\\n",
       "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
       "mean     3.884365  122.502463   72.519591   29.392111  156.279874   32.596053   \n",
       "std      3.396762   30.833718   11.955500    8.914926   87.090249    7.009831   \n",
       "min      0.000000   44.000000   24.000000    7.000000   15.000000   18.200000   \n",
       "25%      1.000000  100.000000   64.000000   25.250000  120.500000   27.500000   \n",
       "50%      3.000000  119.000000   72.519591   29.392111  156.279874   32.400000   \n",
       "75%      6.000000  142.000000   80.000000   33.000000  156.279874   36.875000   \n",
       "max     17.000000  198.000000  114.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "             pedi         age  \n",
       "count  614.000000  614.000000  \n",
       "mean     0.480606   33.285016  \n",
       "std      0.331490   11.698435  \n",
       "min      0.078000   21.000000  \n",
       "25%      0.248000   24.000000  \n",
       "50%      0.384000   29.000000  \n",
       "75%      0.639250   40.000000  \n",
       "max      2.329000   81.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo\n",
    "\n",
    "# El siguiente imputer sustituye los np.nan por la media (Valores por defecto)\n",
    "# Primero definimos el modelo\n",
    "imp = Imputer()\n",
    "\n",
    "# Ahora lo entrenamos\n",
    "imp = imp.fit(dfsh['pima']['train']['atts'])\n",
    "\n",
    "# Finalmente lo podemos usar para transformar el dataframe de la siguiente forma\n",
    "X = imp.transform(dfsh['pima']['train']['atts'])\n",
    "Y = imp.transform(dfsh['pima']['test']['atts'])\n",
    "\n",
    "# El resultado es una matriz, por lo que debemos transformarla de nuevo en un dataframe\n",
    "train_attsFull = pd.DataFrame(X, columns = dfsh['pima']['train']['atts'].columns)\n",
    "test_attsFull = pd.DataFrame(Y, columns = dfsh['pima']['test']['atts'].columns)\n",
    "\n",
    "train_attsFull.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma tenemos que por parte del Imputer las variables a configurar son:\n",
    "* missing_values: variable con el valor con el que se codifican los valores perdidos. Por defecto missing_values = 'NaN', que sustituye los np.nan.\n",
    "* strategy: variable que indica con qué se reemplazan los valores perdidos. Puede ser: 'mean', 'median' y 'most_frequent'. Por defecto strategy = 'mean'.\n",
    "* axis: eje en el que se hace la imputación (0 para columnas, 1 para filas). Por defecto axis = 0.\n",
    "\n",
    "La información ha sido obtenida de la documentación encontrada en [sklearn.preprocessing.Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html#sklearn.preprocessing.Imputer.fit_transform)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez terminado el tratamiento de valores perdidos, podemos proceder a crear los clasificadores. En esta práctica se utilizarán dos métodos de clasificación: KNN y árbol de decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Clasificador KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN es un clasificador sencillo de entender y configurar, ya que solo tendremos que fijar el parámetro `k` que determina el número de vecinos con los que compararemos.\n",
    "\n",
    "Se trata de un algoritmo perezoso, es decir, que no realiza fase de aprendizaje previa porque computa los parámetros necesarios para la clasificación durante el propio proceso de clasificación. Aunque esto pueda parecer una ventaja puede llegar a resultar ineficiente para bases de datos con muchas instancias. Además, es muy sensible a cambios en los datos de training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "\n",
    "# Se prueba el clasificador con un valor de k = 5 vecinos sin pesar por la distancia (Por defecto).\n",
    "# La distancia usada es la de Minkowski\n",
    "model = neighbors.KNeighborsClassifier()\n",
    "# Como es perezoso solo se inicializa su estado.\n",
    "\n",
    "# A continuacion se aprenden los datos del conjunto de training.\n",
    "knn = model.fit(train_attsFull, dfsh['pima']['train']['label'])\n",
    "\n",
    "predictionKNN = knn.predict(test_attsFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma tenemos que por parte del clasificador KNN los hiperparámetros a configurar son:\n",
    "* n_neighbors: variable con el número de vecinos usados. Por defecto n_neighbors = 5. Es la K de KNN.\n",
    "* weights: variable que indica cómo pesar los vecinos a la hora de clasificar, pudiendo elegir entre que todos pesen lo mismo ('uniform') o pesar por la inversa de la distancia ('distance'). También es posible pasar por parámetro una función que devuelva el peso recibiendo como argumento un array de distancias. Por defecto weights = 'uniform'.\n",
    "* metric: métrica de distancia a utilizar. Las posibles métricas pueden encontrarse en [class DistanceMetric](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html). Por defecto metric = 'minkowski'.\n",
    "* p: parámetro para la métrica de distancia 'minkowsky', donde la distancia entre x e y se calcula como sum(|x - y|^p)^(1/p). Por ejemplo para p = 1 es la distancia de Manhattan y con p = 2 es la Euclídea. Por defecto p = 2.\n",
    "* metric_params: otros argumentos que pueden ser usados en la función de distancia escogida. Por ejemplo en la distancia de Mahalanobis o en la de WMinkowsky.\n",
    "\n",
    "Hay más variables que sirven para mejorar la eficiencia pero que no afectan al resultado de la clasificación, que afectan por ejemplo al nivel de concurrencia. Por ello, no las trataremos de momento.\n",
    "\n",
    "La información ha sido obtenida de la documentación encontrada en [sklearn.neighbors.KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica anterior ya se trabajó con árboles de decisión, por lo que ahora nos centraremos más en los hiperparámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "\n",
    "# Iniciamos el modelo usando la impureza Gini como criterio para las divisiones, sin profundidad máxima.\n",
    "model = tree.DecisionTreeClassifier(random_state = seed)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "classifier = model.fit(train_attsFull, dfsh['pima']['train']['label'])\n",
    "\n",
    "# Obtenemos la predicción\n",
    "prediction = classifier.predict(test_attsFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma tenemos que por parte del árbol de decisión los hiperparámetros a configurar son:\n",
    "* criterion: variable con el criterio para hacer las divisiones. Puede ser 'gini' para usar la impureza Gini o 'entropy' para usar la ganancia de información. Por defecto criterion = 'gini'.\n",
    "* splitter: variable que permite decidir entre buscar el mejor corte con 'best' o el mejor corte aleatorio con 'random'. Por defecto splitter = 'best'.\n",
    "* max_depth: variable que establece la profundidad máxima del árbol. Reducirla puede ayudar a evitar el sobreajuste. Por defecto max_depth = None.\n",
    "* min_samples_split: variable que permite determinar el mínimo número de instancias necesarias para dividir un nodo interno. Aumentar el número permite disminuir el sobreajuste. Por defecto min_samples_split = 2.\n",
    "* min_samples_leaf: variable que permite determinar el mínimo número de casos necesarios para que un nodo sea hoja. Aumentar el número permite disminuir el sobreajuste. Por defecto min_samples_leaf = 1.\n",
    "* min_weight_fraction_leaf: se usa cuando a la hora de entrenar se han especificado pesos para los distintos casos. De no especificarse todos los casos tendrán el mismo peso. La variable indica la minima fracción de peso del total de pesos necesaria para que un nodo pueda ser hoja. Por defecto min_weight_fraction_leaf = 0. (debe ser float).\n",
    "* max_features: el número de variables a considerar a la hora de buscar el mejor corte. Se puede especificar un entero, un flotante con el porcentaje, 'auto' o 'sqrt' para tomar la raíz del número total de variables (¿por qué auto hace siempre lo mismo?), 'log2' para tomar el logaritmo en base 2 o None para tomar max_features = numero total de variables. Por defecto max_features = None.\n",
    "* max_leaf_nodes: máximo número de nodos hoja, donde el árbol se construirá con primero-mejor pesando con la impureza. Por defecto max_leaf_nodes = None.\n",
    "* min_impurity_decrease: un nodo se divide si al hacerlo se produce una disminución de impureza mayor o igual a éste valor. Aumentar el valor disminuye el sobreajuste. Por defecto min_impurity_decrease = 0.\n",
    "* class_weight: indica el peso de cada etiqueta posible de la variable clase (también puede usarse en problemas multiclase). Puede pasarse un diccionario etiqueta: peso, una lista de diccionarios, 'balanced' para que pese las etiquetas en función de la inversa de sus frecuencias de aparición o None para que todas tengan el mismo peso. Por defecto class_weight = None.\n",
    "\n",
    "Hay más hiperparámetros que sirven para mejorar la eficiencia pero que no afectan al resultado de la clasificación, como presort. También tenemos el hiperparámetro random_state, para introducir la semilla. Por ello, no las trataremos de momento.\n",
    "\n",
    "La información ha sido obtenida de la documentación encontrada en [sklearn.tree.DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values = '0', strategy = 'mean', axis = 0)\n",
    "imp.fit(dfs[pima])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
